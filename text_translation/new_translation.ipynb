{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "new_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjVpP_kv4yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "32988aae-0954-4d4e-9b24-c2e0dbcdd89c"
      },
      "source": [
        "# mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zb93R3DCCXK0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "aeb0ad3f-9f00-46d8-d660-02e9a4493db1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector, Dropout, Bidirectional, Input, dot, Activation, TimeDistributed, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import tensorflow.keras \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import io\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# plot japanese lang\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "!pip install keras-multi-head\n",
        "from keras_multi_head import MultiHead\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/c0/b75d434be51a8cc11d2e9b36f2d7f93a1bcf63bde24dc79a61d329d60b2a/japanize-matplotlib-1.0.5.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 3.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.0.5-cp36-none-any.whl size=4118721 sha256=5a50229687d53e8d91a6a9656ada0ca0f278583fc0a669775873678915553034\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/8a/08/4a784957da9f3c2b4839b4986be2fba2a481877318948be52c\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.0.5\n",
            "Collecting keras-multi-head\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.17.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.2.5)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.8.0)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=7886e366c2f60647c60049446715704babfa99d1110cd34ebd05d080224ddf18\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=652173e5b672179a02987975cc35f3a702ad04a164c4c90f76250c1f6c680e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "Successfully installed keras-multi-head-0.22.0 keras-self-attention-0.41.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm1CEgijFK_B",
        "colab_type": "text"
      },
      "source": [
        "# load text file\n",
        "\n",
        "**this dataset is aleady implemented a SentenceSpace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmv0RqrACbd-",
        "colab": {}
      },
      "source": [
        "num_example = 50000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"/content/drive/My Drive/Colab Notebooks/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGytFt5qxDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl22ZyPk2PfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use other dataset\n",
        "en = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.en\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "ja = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.ja\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "en = en[:30000]\n",
        "ja = ja[:30000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnsuu4-gzjB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f0fe7699-a457-422a-9584-f7e6c05ac7c0"
      },
      "source": [
        "# sentencepiece\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model. load(\"/content/drive/My Drive/wiki-ja.model\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 1.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtoY2Kl2Jai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  ja = [\" \".join(spm_model.encode_as_pieces(i)).replace(\"▁\", \"\") for i in ja]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhEAtSYFSys",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        "**Removing accented characters** \n",
        "\n",
        "e.g. é → e.\n",
        "\n",
        "**Expanding Contractions** \n",
        "\n",
        "e.g. don't → do not, I'd → I would\n",
        "\n",
        "**remove special word** \n",
        "\n",
        "e.g. remove \"123#@\"\n",
        "\n",
        "**Stemming**\n",
        "\n",
        " e.g. corder, codes → code\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        " e.g. better → good\n",
        "\n",
        "**Tokenize**\n",
        "\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS0AX_jOFk_l",
        "colab_type": "text"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ttcy6dKhDrmB",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OMHI0M1qPWqa",
        "colab": {}
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO9i_n__FrHg",
        "colab_type": "text"
      },
      "source": [
        "# Expanding Contractions\n",
        "Japanese doesn't have a Contraction words so I just create a one function to expand Contractions for Engish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUEU_GU1PYNq",
        "colab": {}
      },
      "source": [
        "def expand_constraction(text):\n",
        "\n",
        "    #  dic for expand constraction words\n",
        "    constraction_dict= {\n",
        "        \"ain't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he he will have\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'd've\": \"I would have\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'd've\": \"i would have\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'll've\": \"i will have\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"needn't've\": \"need not have\",\n",
        "        \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"shan't've\": \"shall not have\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\",\n",
        "        \"so've\": \"so have\",\n",
        "        \"so's\": \"so as\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\",\n",
        "        \"there'd've\": \"there would have\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\",\n",
        "        \"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\n",
        "        \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'd've\": \"you would have\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    #  define match pattern\n",
        "    #  IGNORECASE → no matter if word is lowercase or uppercase\n",
        "    #  DOTAIL → . is going to match \\n\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(constraction_dict.keys())),\n",
        "                                                  flags=re.IGNORECASE | re.DOTALL)\n",
        "    #  expand words\n",
        "    def expand_match(constraction):\n",
        "        # get constraction word\n",
        "        match = constraction.group(0)\n",
        "        first_char = match[0]\n",
        "        #  get expand word from constraction dict\n",
        "        expand_constraction = constraction_dict.get(match)\\\n",
        "                                                    if constraction_dict.get(match) \\\n",
        "                                                    else constraction_dict.get(match.lower())\n",
        "        \n",
        "        # create expand constraction\n",
        "        expand_constraction = first_char + expand_constraction[1:]\n",
        "        return expand_constraction\n",
        "    \n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ikqxXGePadk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "416a7575-53fa-4bc9-b5f4-257d67179415"
      },
      "source": [
        "# e.g.\n",
        "expand_constraction(\"you're good I'd like to go he's she's\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you are good I would like to go he is she is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZrYrRxFwc8",
        "colab_type": "text"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \"?\", \"!\", \"-\", \"ー\", \"Kanji\", \"Katakana\", \"Hiragana\") \n",
        "create space between word and punctuation (? ! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBMqrm3o-sJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space(text):\n",
        "    pattern = r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\-/\\s]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = re.sub(r\"([?!.,。、])\", r\" \\1 \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeOvXEYFQq_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08c800f6-1bf1-4b20-c2e5-dfdb2c67ab81"
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello, . #@…123world.\"), replace_special_character_to_space(\"こん・にちは。・ いい天気。\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hello  world', 'こんにちは いい天気')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-z-PhGGQqL",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "I will do stemming only english which can create a base form of a word from a given word. Japanese language doesn't need a stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYiW2_Pl1lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemmer_word(text):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-iz13U2awVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48a351f1-3522-4875-e84f-6d109d082f91"
      },
      "source": [
        "# e.g.\n",
        "stemmer_word(\"hello world she has cat but he had dogs he is went to traveling\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world she ha cat but he had dog he is went to travel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqW5RHMGifD",
        "colab_type": "text"
      },
      "source": [
        "# Normalize each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sizAmLv2Pj-j",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = ()\n",
        "    target_value = ()\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space(en_text)\n",
        "         \n",
        "        input_value += (en_text, )\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = expand_constraction(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "\n",
        "        ja_text = \"START_ \" + ja_text + \" _END\"\n",
        "        \n",
        "        target_value += (ja_text, )\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMa52HiWGtQ-",
        "colab_type": "text"
      },
      "source": [
        "# get clean text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1WLwCK4xVynu",
        "colab": {}
      },
      "source": [
        "input_value, target_value = normalize_english(en, ja)\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLVeltmkUN1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06119913-6a0a-4fc7-973d-b6fafb21b157"
      },
      "source": [
        "all_eng = set()\n",
        "for inp in x:\n",
        "  for word in inp.split():\n",
        "    if word not in all_eng:\n",
        "      all_eng.add(word)\n",
        "\n",
        "all_ja = set()\n",
        "for tar in y:\n",
        "  for word_ja in tar.split():\n",
        "    if word_ja not in all_ja:\n",
        "      all_ja.add(word_ja)\n",
        "\n",
        "\n",
        "print(\"unique vocab %d\" % len(all_eng))\n",
        "print(\"unique vocab %d\" %len(all_ja))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique vocab 12307\n",
            "unique vocab 11371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFPoVYpV8dP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d40903-0cf9-4dab-9507-807fff74d29b"
      },
      "source": [
        "input_word = sorted(list(all_eng))\n",
        "target_word = sorted(list(all_ja))\n",
        "\n",
        "num_encoder_tokens = len(all_eng)\n",
        "num_decoder_tokens = len(all_ja)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12307, 11371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4zaIQbZa7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e61a2253-1871-47c3-a33a-69a454f112a2"
      },
      "source": [
        "num_decoder_tokens+=1\n",
        "print(num_decoder_tokens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFCjZpvyWnpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([ (word, i+1) for i, word in enumerate(input_word)])\n",
        "target_token_index = dict([ (word, i+1) for i, word in enumerate(target_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiVPd5RXHHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryw13tZPNMpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "4a23a0df-7a63-4764-f3a0-1ca3e59ec659"
      },
      "source": [
        "# populate the lists with sentence lengths\n",
        "english_len = [len(i.split()) for i in x]\n",
        "\n",
        "japanese_len = [len(i.split()) for i in y]\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(english_len)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(japanese_len)\n",
        "\n",
        "# print max length\n",
        "print(\"english length:\", max(english_len))\n",
        "print(\"japanese length:\", max(japanese_len))\n",
        "max_len_input =  max(english_len)\n",
        "max_len_target =  max(japanese_len)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 54\n",
            "japanese length: 49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdiAdGYR97Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WemPmLVUI7X8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca706e05-8049-49f8-96b0-8e37869b5fc9"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCwdO3dY-mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X = X_train, Y = Y_train, batch_size=128):\n",
        "  while True:\n",
        "      assert len(X) == len(Y)\n",
        "      for i in range(0, len(X), batch_size):\n",
        "          encoder_input_data = np.zeros((batch_size, max_len_input), dtype=\"float32\")\n",
        "          decoder_input_data = np.zeros((batch_size, max_len_target), dtype=\"float32\")\n",
        "          decoder_target_data = np.zeros((batch_size, max_len_target, num_decoder_tokens), dtype=\"float32\")\n",
        "          for j, (input_text, target_text) in enumerate(zip(X[i:i+batch_size], Y[i:i+batch_size])):\n",
        "              for t, word in enumerate(input_text.split()):\n",
        "                  encoder_input_data[j, t] = input_token_index[word]\n",
        "        \n",
        "              for t, word in enumerate(target_text.split()):\n",
        "                  if t < len(target_text.split())-1:\n",
        "                      decoder_input_data[j, t] = target_token_index[word]\n",
        "          \n",
        "                  if t > 0:\n",
        "                      decoder_target_data[j, t-1, target_token_index[word]] = 1.\n",
        "          yield([encoder_input_data, decoder_input_data], decoder_target_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215DTscueVam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build NMT model\n",
        "in_timesteps = max_len_input\n",
        "out_timesteps = max_len_target\n",
        "units = 512\n",
        "\n",
        "# build encoder model\n",
        "encoder_input = Input(shape=(in_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "encoder_embedding = Embedding(input_dim=num_encoder_tokens+1, output_dim=units, mask_zero=True)(encoder_input) \n",
        "# Dropout\n",
        "encoder_embedding = Dropout(0.2)(encoder_embedding)\n",
        "\n",
        "encoder_output, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "test_encoder_state = [encoder_output, state_h, state_c ]\n",
        "\n",
        "# build decoder model\n",
        "decoder_input = Input(shape=(out_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=units, mask_zero=True)\n",
        "decoder_emb = decoder_embedding(decoder_input)\n",
        "\n",
        "# Dropout\n",
        "decoder = Dropout(0.2)(decoder_emb)\n",
        "\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder, _, _ = decoder_lstm(decoder,  initial_state=encoder_states)\n",
        "\n",
        "\n",
        "t = Dense(5000, activation='tanh')(decoder)\n",
        "t2 = Dense(5000, activation='tanh')(encoder_output)\n",
        "attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "attention = Dense(in_timesteps, activation='tanh')(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "\n",
        "context = dot([attention, encoder_output], axes = [2,1])\n",
        "\n",
        "decoder_combined_context = tensorflow.keras.layers.concatenate([context, decoder])\n",
        "\n",
        "decoder_combined_context=Dense(2000, activation='tanh')(decoder_combined_context)\n",
        "output=(Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_combined_context)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXLs9SDRhIy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_train = generate_batch(X_train, Y_train, 128)\n",
        "gen_val = generate_batch(X_test, Y_test, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6j01NDZj0Ce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3efc6256-4db9-4e20-fbdf-45c5052de148"
      },
      "source": [
        "from math import ceil\n",
        "trian_step = ceil(len(X_train) / 128 )\n",
        "val_step = ceil(len(X_test) / 128)\n",
        "val_step"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opmqFdDziBng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO6Hd4RftoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24dd3239-9757-425c-b4ad-7f5306da27fe"
      },
      "source": [
        "adam = optimizers.Adam(lr=0.0001, decay=1e-2)\n",
        "\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', precision])\n",
        "model.summary()\n",
        "# checkpoint\n",
        "filename = 'model.h1.22_Nov_19'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', save_best_only=True, mode='min')              \n",
        "\n",
        "# train model\n",
        "history = model.fit_generator(gen_train, validation_data=gen_val , steps_per_epoch=trian_step, validation_steps=val_step, epochs=5, callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 54)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 49)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 54, 512)      6301696     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 49, 512)      5822464     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 54, 512)      0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 49, 512)      0           embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 54, 512), (N 2099200     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, 49, 512), (N 2099200     dropout_5[0][0]                  \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 49, 5000)     2565000     lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 54, 5000)     2565000     lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, 49, 54)       0           dense_10[0][0]                   \n",
            "                                                                 dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 49, 54)       2970        dot_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 49, 54)       0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 49, 512)      0           activation_2[0][0]               \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 49, 1024)     0           dot_5[0][0]                      \n",
            "                                                                 lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 49, 2000)     2050000     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 49, 11372)    22755372    dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 46,260,902\n",
            "Trainable params: 46,260,902\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "164/165 [============================>.] - ETA: 2s - loss: 1.2359 - acc: 0.1247 - precision: 0.0057Epoch 1/5\n",
            "165/165 [==============================] - 493s 3s/step - loss: 1.2288 - acc: 0.1247 - precision: 0.0057 - val_loss: 1.0504 - val_acc: 0.1282 - val_precision: 3.2508e-04\n",
            "Epoch 2/5\n",
            "164/165 [============================>.] - ETA: 2s - loss: 1.0524 - acc: 0.1277 - precision: 0.0066Epoch 1/5\n",
            "165/165 [==============================] - 487s 3s/step - loss: 1.0464 - acc: 0.1277 - precision: 0.0066 - val_loss: 1.0389 - val_acc: 0.1282 - val_precision: 0.0158\n",
            "Epoch 3/5\n",
            "164/165 [============================>.] - ETA: 2s - loss: 1.0414 - acc: 0.1318 - precision: 0.0227Epoch 1/5\n",
            "165/165 [==============================] - 487s 3s/step - loss: 1.0354 - acc: 0.1318 - precision: 0.0227 - val_loss: 1.0327 - val_acc: 0.1380 - val_precision: 0.0245\n",
            "Epoch 4/5\n",
            "164/165 [============================>.] - ETA: 2s - loss: 1.0342 - acc: 0.1393 - precision: 0.0244Epoch 1/5\n",
            "165/165 [==============================] - 489s 3s/step - loss: 1.0284 - acc: 0.1392 - precision: 0.0244 - val_loss: 1.0281 - val_acc: 0.1410 - val_precision: 0.0241\n",
            "Epoch 5/5\n",
            "164/165 [============================>.] - ETA: 2s - loss: 1.0287 - acc: 0.1417 - precision: 0.0241Epoch 1/5\n",
            "165/165 [==============================] - 497s 3s/step - loss: 1.0228 - acc: 0.1417 - precision: 0.0241 - val_loss: 1.0247 - val_acc: 0.1427 - val_precision: 0.0238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55uxIDrTSvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "11df81eb-25ea-407d-886b-eebf18b74120"
      },
      "source": [
        "# define encoder model\n",
        "encoder_model = Model(encoder_input, test_encoder_state)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 54)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 54, 512)           6301696   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 54, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                [(None, 54, 512), (None,  2099200   \n",
            "=================================================================\n",
            "Total params: 8,400,896\n",
            "Trainable params: 8,400,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4Rk1j6LwGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_decoder(encoder_out):\n",
        "    # define decoder architecuture\n",
        "    decoder_state_h = Input(shape=(units, ))\n",
        "    decoder_state_c = Input(shape=(units, ))\n",
        "    decoder_state = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "    # decoder embedding\n",
        "    dec_emb2 = decoder_embedding(decoder_input)\n",
        "    decoder2 = Dropout(0.1)(dec_emb2)\n",
        "\n",
        "    decoder_output, state_h2, state_c2 = decoder_lstm(decoder2, initial_state=decoder_state)\n",
        "\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "    # t = Dense(5000, activation='tanh')(decoder_output)\n",
        "    # t2 = Dense(5000, activation='tanh')(encoder_out)\n",
        "    # attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "    # attention = Dense(in_timesteps, activation='tanh')(attention)\n",
        "    # attention = Activation('softmax')(attention)\n",
        "\n",
        "    # context = dot([attention, encoder_out], axes = [2,1])\n",
        "\n",
        "    # decoder_combined_context = keras.layers.concatenate([context, decoder_output])\n",
        "\n",
        "    # decoder_combined_context=Dense(2000, activation='tanh')(decoder_combined_context)\n",
        "\n",
        "    decoder_output = (Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_output)\n",
        "\n",
        "    decoder_model = Model([decoder_input] + decoder_state, [decoder_output] + decoder_states2)\n",
        "\n",
        "    return decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Zf2eAyTejb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_seq(input_seq):\n",
        "    # encoder the input seq as vector\n",
        "    output_en, a, b = encoder_model.predict(input_seq)\n",
        "    enc_output = encoder_model.layers[3].output[0]\n",
        "    en_state = [a, b]\n",
        "    # generate empty target sequence\n",
        "    target_seq = np.zeros((1, 49))\n",
        "    # populate the first character of target seq\n",
        "    target_seq[0, 0] = target_token_index[\"START_\"]\n",
        "\n",
        "    # loop for batch of sequences\n",
        "    stop_condition = False\n",
        "    decoder_sentence = ''\n",
        "    decoder_model = define_decoder(enc_output)\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_token, h, c = decoder_model.predict([target_seq] + en_state)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_token[0, -1, :])\n",
        "        sampled_char = reverse_target_char[sampled_token_index]\n",
        "        decoder_sentence += ' ' + sampled_char\n",
        "\n",
        "\n",
        "        # stop condition\n",
        "        if sampled_char == \"_END\" or len(decoder_sentence) > 49:\n",
        "          stop_condition = True\n",
        "        \n",
        "\n",
        "        # update the target sequence\n",
        "        target_seq = np.zeros((1, 49))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # update states\n",
        "        en_state = [h, c]\n",
        "    \n",
        "    return decoder_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwXPOWqZIkik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_train, Y_train, 1)\n",
        "k = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZMb7fNtfLhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae07860e-5d2f-4b3e-cd93-10bc3c9f79da"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoder_sentence = decoder_seq(input_seq)\n",
        "print(\"input english %s\" % X_train[k:k+1].values[0])\n",
        "print(\"actual %s\" % Y_train[k:k+1].values[0][6:-4])\n",
        "print(\"predict %s\" % decoder_sentence[:-4] )"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input english i was tryin to stabilise below the belt when this guy the size of the statue of liberty walks up to me\n",
            "actual  下 を な だ めて た 時に  自由 の 女神 のような  デ カ 物 が やってきた \n",
            "predict  戦闘機 が変わる が変わる ヘルメット ヘルメット ヘルメット ヘルメット ヘルメット ヘ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqoRE4e7KWJc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2RMYOGXKWDe",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# calculate BLEU score\n",
        "print('BLEU corpus: %f' % corpus_bleu(actual, predicted))\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# print(\"BLUE  sentense: %f\" % sentence_bleu(actual, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_tOYFO-kFuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2tR3wEMNxGQ",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRuUlEkw9tub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42aD6GUb8c4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkIz16eXghfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZCZbh19lkz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE-iyAjAVWCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tg2abLBPZTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifn_uRr0RNNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}