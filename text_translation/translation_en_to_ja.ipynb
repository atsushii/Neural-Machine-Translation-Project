{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "translation_en_to_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjVpP_kv4yc",
        "colab_type": "code",
        "outputId": "ecd4812c-98db-48c6-c60a-6bcb78e83caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zb93R3DCCXK0",
        "outputId": "978496cf-cfd3-45b5-a5d6-99a3a08508b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import io\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# plot japanese lang\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/c0/b75d434be51a8cc11d2e9b36f2d7f93a1bcf63bde24dc79a61d329d60b2a/japanize-matplotlib-1.0.5.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 101kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.0.5-cp36-none-any.whl size=4118721 sha256=3a5ded069f494c1e8f6f98b2aa4016d9ea300d2e3bc08c968c59811ab8800850\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/8a/08/4a784957da9f3c2b4839b4986be2fba2a481877318948be52c\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm1CEgijFK_B",
        "colab_type": "text"
      },
      "source": [
        "# load text file\n",
        "\n",
        "**this dataset is aleady implemented a SentenceSpace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmv0RqrACbd-",
        "colab": {}
      },
      "source": [
        "num_example = 10000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"/content/drive/My Drive/Colab Notebooks/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGytFt5qxDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl22ZyPk2PfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use other dataset\n",
        "en = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.en\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "ja = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.ja\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "en = en[:30000]\n",
        "ja = ja[:30000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnsuu4-gzjB6",
        "colab_type": "code",
        "outputId": "f0fe7699-a457-422a-9584-f7e6c05ac7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# sentencepiece\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model. load(\"/content/drive/My Drive/wiki-ja.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 1.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtoY2Kl2Jai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # replace ▁ to ''\n",
        "  ja = [\" \".join(spm_model.encode_as_pieces(i)).replace(\"▁\", \"\") for i in ja]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhEAtSYFSys",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        "**Removing accented characters** \n",
        "\n",
        "e.g. é → e.\n",
        "\n",
        "**Expanding Contractions** \n",
        "\n",
        "e.g. don't → do not, I'd → I would\n",
        "\n",
        "**remove special word** \n",
        "\n",
        "e.g. remove \"123#@\"\n",
        "\n",
        "**Stemming**\n",
        "\n",
        " e.g. corder, codes → code\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        " e.g. better → good\n",
        "\n",
        "**Tokenize**\n",
        "\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS0AX_jOFk_l",
        "colab_type": "text"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ttcy6dKhDrmB",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OMHI0M1qPWqa",
        "outputId": "db70be9c-1309-40c2-8206-b71ed9e344fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('こんにちは。今日は', 'Hello world e ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO9i_n__FrHg",
        "colab_type": "text"
      },
      "source": [
        "# Expanding Contractions\n",
        "Japanese doesn't have a Contraction words so I just create a one function to expand Contractions for Engish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUEU_GU1PYNq",
        "colab": {}
      },
      "source": [
        "def expand_constraction(text):\n",
        "\n",
        "    #  dic for expand constraction words\n",
        "    constraction_dict= {\n",
        "        \"ain't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he he will have\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'd've\": \"I would have\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'd've\": \"i would have\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'll've\": \"i will have\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"needn't've\": \"need not have\",\n",
        "        \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"shan't've\": \"shall not have\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\",\n",
        "        \"so've\": \"so have\",\n",
        "        \"so's\": \"so as\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\",\n",
        "        \"there'd've\": \"there would have\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\",\n",
        "        \"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\n",
        "        \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'd've\": \"you would have\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    #  define match pattern\n",
        "    #  IGNORECASE → no matter if word is lowercase or uppercase\n",
        "    #  DOTAIL → . is going to match \\n\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(constraction_dict.keys())),\n",
        "                                                  flags=re.IGNORECASE | re.DOTALL)\n",
        "    #  expand words\n",
        "    def expand_match(constraction):\n",
        "        # get constraction word\n",
        "        match = constraction.group(0)\n",
        "        first_char = match[0]\n",
        "        #  get expand word from constraction dict\n",
        "        expand_constraction = constraction_dict.get(match)\\\n",
        "                                                    if constraction_dict.get(match) \\\n",
        "                                                    else constraction_dict.get(match.lower())\n",
        "        \n",
        "        # create expand constraction\n",
        "        expand_constraction = first_char + expand_constraction[1:]\n",
        "        return expand_constraction\n",
        "    \n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ikqxXGePadk",
        "outputId": "03b291f2-6f26-4d30-d25e-13ef5491d7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "expand_constraction(\"you're good I'd like to go he's she's\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you are good I would like to go he is she is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZrYrRxFwc8",
        "colab_type": "text"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \"?\", \"!\", \"-\", \"ー\", \"Kanji\", \"Katakana\", \"Hiragana\") \n",
        "create space between word and punctuation (? ! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBMqrm3o-sJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space(text):\n",
        "    pattern = r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\-/\\s]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = re.sub(r\"([?!.,。、])\", r\" \\1 \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeOvXEYFQq_-",
        "outputId": "5c0cc7f8-6932-41e7-a997-176dd469421f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello, . #@…123world.\"), replace_special_character_to_space(\"こん・にちは。・ いい天気。\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hello  world', 'こんにちは いい天気')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-z-PhGGQqL",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "I will do stemming only english which can create a base form of a word from a given word. Japanese language doesn't need a stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYiW2_Pl1lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemmer_word(text):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-iz13U2awVV",
        "colab_type": "code",
        "outputId": "5b367cb1-a04a-4524-a15e-2abaeaf92262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "stemmer_word(\"hello world she has cat but he had dogs he is went to traveling\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world she ha cat but he had dog he is went to travel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqW5RHMGifD",
        "colab_type": "text"
      },
      "source": [
        "# Normalize each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sizAmLv2Pj-j",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = ()\n",
        "    target_value = ()\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space(en_text)\n",
        "\n",
        "        # input value doesn't need  a START and END sentence  \n",
        "        input_value += (en_text, )\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = expand_constraction(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "\n",
        "        # add StTART and END sentence\n",
        "        ja_text = \"START_ \" + ja_text + \" _END\"\n",
        "        \n",
        "        target_value += (ja_text, )\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMa52HiWGtQ-",
        "colab_type": "text"
      },
      "source": [
        "# get clean text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1WLwCK4xVynu",
        "colab": {}
      },
      "source": [
        "# get normalize text data\n",
        "input_value, target_value = normalize_english(en, ja)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xAuLWUomimI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dd109e14-88a4-471f-a27f-441a86818f40"
      },
      "source": [
        "pd.DataFrame({\"input\": x, \"target\": y}).head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>you are back are not you harold ?</td>\n",
              "      <td>START_ あなたは戻ったのね ハロルド ?  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my opponent is shark</td>\n",
              "      <td>START_ 俺の相手は シャークだ _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is one thing in exchange for another</td>\n",
              "      <td>START_ 引き換えだ ある事とある物の _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yeah i am fine</td>\n",
              "      <td>START_ もういいよ ごちそうさま ううん _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do not come to the office anymore do not call ...</td>\n",
              "      <td>START_ もう会社には来ないでくれ 電話もするな _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input                            target\n",
              "0                 you are back are not you harold ?     START_ あなたは戻ったのね ハロルド ?  _END\n",
              "1                               my opponent is shark          START_ 俺の相手は シャークだ _END\n",
              "2          this is one thing in exchange for another       START_ 引き換えだ ある事とある物の _END\n",
              "3                                     yeah i am fine     START_ もういいよ ごちそうさま ううん _END\n",
              "4  do not come to the office anymore do not call ...  START_ もう会社には来ないでくれ 電話もするな _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLVeltmkUN1B",
        "colab_type": "code",
        "outputId": "dfe7596e-ce53-4b65-f237-1cd09c25929f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get unique vocab\n",
        "\n",
        "# input value \n",
        "all_eng = set()\n",
        "for input_lang in x:\n",
        "    for word in input_lang.split():\n",
        "        if word not in all_eng:\n",
        "            all_eng.add(word)\n",
        "\n",
        "# target value\n",
        "all_ja = set()\n",
        "for target_lang in y:\n",
        "    for word in target_lang.split():\n",
        "        if word not in all_ja:\n",
        "            all_ja.add(word)\n",
        "\n",
        "print(\"unique vocab %d\" % len(all_eng))\n",
        "print(\"unique vocab %d\" %len(all_ja))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique vocab 8667\n",
            "unique vocab 14897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5xXXCao7sm",
        "colab_type": "code",
        "outputId": "b7b55115-7cc9-4ad6-aa3c-f8caca22d024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "# max length of input sentense and target sentense\n",
        "english_len = [len(i.split()) for i in x]\n",
        "\n",
        "japanese_len = [len(i.split()) for i in y]\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(english_len)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(japanese_len)\n",
        "\n",
        "# print max length\n",
        "print(\"english length:\", max(english_len))\n",
        "print(\"japanese length:\", max(japanese_len))\n",
        "max_len_input =  max(english_len)\n",
        "max_len_target =  max(japanese_len)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 40\n",
            "japanese length: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJakhhr4kNcTQl6SG/H+F4MDj2eWrwQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFPoVYpV8dP",
        "colab_type": "code",
        "outputId": "347a0931-4527-4235-ecc2-4714017a2cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_word = sorted(list(all_eng))\n",
        "target_word = sorted(list(all_ja))\n",
        "\n",
        "num_encoder_tokens = len(all_eng)\n",
        "num_decoder_tokens = len(all_ja)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8667, 14897)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4zaIQbZa7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_decoder_tokens += 1\n",
        "num_encoder_tokens += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFCjZpvyWnpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([ (word, i+1) for i, word in enumerate(input_word)])\n",
        "target_token_index = dict([ (word, i+1) for i, word in enumerate(target_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiVPd5RXHHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Z1rdVHLLuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7638926-e930-480d-9b63-962be501113a"
      },
      "source": [
        "len(y) * max_len_target*num_decoder_tokens"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1787760000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dws3DMnEQ1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize\n",
        "encoder_input_data = np.zeros((len(x), max_len_input), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(y), max_len_target), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(y), max_len_target, num_decoder_tokens), dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ZjU8GOHfSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in  enumerate(zip(x, y)):\n",
        "    # create encoder input\n",
        "    for t, word in  enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t] = input_token_index[word]\n",
        "    # create decoder input\n",
        "    for  t, word in enumerate(target_text.split()):\n",
        "        if t < len(target_text.split()) - 1:\n",
        "            decoder_input_data[i, t] = target_token_index[word]\n",
        "        if t > 0:\n",
        "          # decoder target is ahead by one time step\n",
        "          # doesn't include start character\n",
        "          # will be one-hot encode\n",
        "          decoder_target_data[i, t - 1, target_token_index[word]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXNxkRB9qoTU",
        "colab_type": "text"
      },
      "source": [
        "# split train data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdiAdGYR97Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WemPmLVUI7X8",
        "colab_type": "code",
        "outputId": "abf96d03-f5d1-46fe-f107-14e939307e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9800,), (6000,), (4200,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282RRGIkq01E",
        "colab_type": "text"
      },
      "source": [
        "# Create generater"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCwdO3dY-mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a batch of data\n",
        "def generate_batch(X = X_train, Y = Y_train, batch_size=64):\n",
        "    while True:\n",
        "        assert len(X) == len(Y)\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            # initialize input and target data\n",
        "            encoder_input_data = np.zeros((batch_size, max_len_input), dtype=\"float32\")\n",
        "            decoder_input_data = np.zeros((batch_size, max_len_target), dtype=\"float32\")\n",
        "            decoder_target_data = np.zeros((batch_size, max_len_target, num_decoder_tokens), dtype=\"float32\")\n",
        "            for j, (input_text, target_text) in enumerate(zip(X[i: i+batch_size], Y[i: i+batch_size])):\n",
        "                    # create enocder input dataset\n",
        "                    for t, word in enumerate(input_text.split()):\n",
        "                        encoder_input_data[j, t] = input_token_index[word]\n",
        "                    # create decoder input dataset\n",
        "                    for t, word in enumerate(target_text.split()):\n",
        "                        if t < len(target_text.split())-1:\n",
        "                            decoder_input_data[j, t] = target_token_index[word]\n",
        "                        # decoder  output is one-hot encoding\n",
        "                        # doesn't include START sentense\n",
        "                        # one timestep ahead of input \n",
        "                        if t > 0:\n",
        "                            decoder_target_data[j, t-1, target_token_index[word]] = 1.\n",
        "                    yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOzbWmLnZ66-",
        "colab_type": "text"
      },
      "source": [
        "# Define for params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYUC0fMZ6fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initial hyper parameter\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "units = 512\n",
        "lr = 0.001\n",
        "l2 = 0.001\n",
        "optimizer = \"adam\"\n",
        "activation = \"tanh\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Vtvs3hvt2Z",
        "colab_type": "text"
      },
      "source": [
        "# Build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215DTscueVam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "45effaa6-9b92-4c0d-9d2b-f9895535ce27"
      },
      "source": [
        "# define time step and dimention size for embedding and LSTM\n",
        "in_timesteps = max_len_input\n",
        "out_timesteps = max_len_target\n",
        "\n",
        "# build encoder model\n",
        "encoder_input = Input(shape=(in_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=units, mask_zero=True)(encoder_input) \n",
        "\n",
        "# Dropout\n",
        "encoder_embedding = Dropout(dropout)(encoder_embedding)\n",
        "\n",
        "# get enternal state to predict target value\n",
        "# encode output is going to use for attention\n",
        "encoder_output, state_h, state_c = LSTM(units, return_sequences=True, return_state=True, recurrent_regularizer=regularizers.l2(l=l2))(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "# it will use for test model\n",
        "test_encoder_state = [encoder_output, state_h, state_c ]\n",
        "\n",
        "# build decoder model\n",
        "decoder_input = Input(shape=(out_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=units, mask_zero=True)\n",
        "decoder_emb = decoder_embedding(decoder_input)\n",
        "\n",
        "# Dropout\n",
        "decoder = Dropout(dropout)(decoder_emb)\n",
        "\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True, recurrent_regularizer=regularizers.l2(l=l2))\n",
        "# we don't use return state in the train model\n",
        "decoder, _, _ = decoder_lstm(decoder,  initial_state=encoder_states)\n",
        "\n",
        "# get attention weight\n",
        "t = Dense(5000, activation='relu')(decoder)\n",
        "t2 = Dense(5000, activation='relu')(encoder_output)\n",
        "attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "attention = Dense(in_timesteps, activation='relu')(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "\n",
        "context = dot([attention, encoder_output], axes = [2,1])\n",
        "\n",
        "decoder_combined_context = tensorflow.keras.layers.concatenate([context, decoder])\n",
        "\n",
        "decoder_combined_context=Dense(2000, activation='relu')(decoder_combined_context)\n",
        "output=(Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_combined_context)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "\n",
        "sgd = optimizers.SGD(lr=lr)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 12)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 40, 512)      4438016     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 12, 512)      7627776     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 40, 512)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 12, 512)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 40, 512), (N 2099200     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 12, 512), (N 2099200     dropout_1[0][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 12, 5000)     2565000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 40, 5000)     2565000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 12, 40)       0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 12, 40)       1640        dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 12, 40)       0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 12, 512)      0           activation[0][0]                 \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 12, 1024)     0           dot_1[0][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 12, 2000)     2050000     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 12, 14898)    29810898    dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 53,256,730\n",
            "Trainable params: 53,256,730\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcIfyzxeRkvb",
        "colab_type": "text"
      },
      "source": [
        "# Gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWzQhS0ZRojg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn=model)\n",
        "\n",
        "param = {\n",
        "    dropout: [0.2, 0.3],\n",
        "    batch_size: [32, 128],\n",
        "    units: [216, 1024],\n",
        "    lr: [0.0001, 0.01, 0.1, 1],\n",
        "    l2: [0.01, 0.05, 0.01],\n",
        "    optimizer: [\"adam\", \"RMSprop\"],\n",
        "    activation: [\"relu\"]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXLs9SDRhIy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataset as generater\n",
        "gen_train = generate_batch(X_train, Y_train, batch_size)\n",
        "gen_val = generate_batch(X_val, Y_val, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6j01NDZj0Ce",
        "colab_type": "code",
        "outputId": "4e592dbf-c034-480d-f8d3-7a093e28d1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# caluclate train step\n",
        "from math import ceil\n",
        "trian_step = ceil(len(X_train) / batch_size )\n",
        "val_step = ceil(len(X_val) / batch_size)\n",
        "val_step"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opmqFdDziBng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO6Hd4RftoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint\n",
        "filename = 'model.h1.22_Nov_19'\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', save_best_only=True, mode='min')              \n",
        "\n",
        "# train model\n",
        "grid = GridSearchCV(estimator=model, param_grid=param, cv=5)\n",
        "grid_result = grid.fit([encoder_input_data, decoder_input_data], decoder_target_data, callbacks=[es])\n",
        "# history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data, validation_split=0.3,\n",
        "#                               callbacks=[es], verbose=1, batch_size=batch_size, epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55uxIDrTSvJ",
        "colab_type": "code",
        "outputId": "b8c37004-5912-4917-9691-bc27b83fa9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# define encoder model\n",
        "encoder_model = Model(encoder_input, test_encoder_state)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 44)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 44, 512)           11392512  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 44, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 44, 512), (None,  2099200   \n",
            "=================================================================\n",
            "Total params: 13,491,712\n",
            "Trainable params: 13,491,712\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4Rk1j6LwGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_decoder(encoder_out):\n",
        "    # define decoder architecuture\n",
        "    decoder_state_h = Input(shape=(units, ))\n",
        "    decoder_state_c = Input(shape=(units, ))\n",
        "    decoder_state = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "    # decoder embedding\n",
        "    dec_emb2 = decoder_embedding(decoder_input)\n",
        "    decoder2 = Dropout(0.1)(dec_emb2)\n",
        "\n",
        "    decoder_output, state_h2, state_c2 = decoder_lstm(decoder2, initial_state=decoder_state)\n",
        "\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "    t = Dense(5000, activation='relu')(decoder_output)\n",
        "    t2 = Dense(5000, activation='relu')(encoder_out)\n",
        "    attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "    attention = Dense(in_timesteps, activation='relu')(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "\n",
        "    context = dot([attention, encoder_out], axes = [2,1])\n",
        "\n",
        "    decoder_combined_context = tensorflow.keras.layers.concatenate([context, decoder_output])\n",
        "\n",
        "    decoder_combined_context=Dense(2000, activation='relu')(decoder_combined_context)\n",
        "\n",
        "    decoder_output = (Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_output)\n",
        "\n",
        "    decoder_model = Model([decoder_input] + decoder_state, [decoder_output] + decoder_states2)\n",
        "\n",
        "    return decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Zf2eAyTejb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_seq(input_seq):\n",
        "    # encoder the input seq as vector\n",
        "    output_en, a, b = encoder_model.predict(input_seq)\n",
        "    enc_output = encoder_model.layers[3].output[0]\n",
        "    en_state = [a, b]\n",
        "    # generate empty target sequence\n",
        "    target_seq = np.zeros((1, max_len_target))\n",
        "    # populate the first character of target seq\n",
        "    target_seq[0, 0] = target_token_index[\"START_\"]\n",
        "\n",
        "    # loop for batch of sequences\n",
        "    stop_condition = False\n",
        "    decoder_sentence = ''\n",
        "    decoder_model = define_decoder(enc_output)\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_token, h, c = decoder_model.predict([target_seq] + en_state)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_token[0, -1, :])\n",
        "        sampled_char = reverse_target_char[sampled_token_index]\n",
        "        decoder_sentence += ' ' + sampled_char\n",
        "\n",
        "\n",
        "        # stop condition\n",
        "        if sampled_char == \"_END\" or len(decoder_sentence) > max_len_input:\n",
        "          stop_condition = True\n",
        "        \n",
        "\n",
        "        # update the target sequence\n",
        "        target_seq = np.zeros((1, max_len_target))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # update states\n",
        "        en_state = [h, c]\n",
        "    \n",
        "    return decoder_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwXPOWqZIkik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_test, Y_test, 1)\n",
        "k = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZMb7fNtfLhc",
        "colab_type": "code",
        "outputId": "45811e2e-0af0-4485-b889-44a83e9eec83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoder_sentence = decoder_seq(input_seq)\n",
        "print(\"input english %s\" % X_test[k:k+1].values[0])\n",
        "print(\"actual %s\" % Y_test[k:k+1].values[0][6:-4])\n",
        "print(\"predict %s\" % decoder_sentence[:-4] )"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input english who is behind the camera flash ? \n",
            "actual  カメラの明かりだ 後ろに誰がいる ?  \n",
            "predict  言うの 飲み直しだよ どうなってしまうか もっと心臓に ヘリが到着しました ヘリが到着\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Lk0FA1TptP",
        "colab_type": "code",
        "outputId": "b82ca947-9bb6-42ef-c73f-ed22f6af1743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_sentence"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' その名前もきっとー 様な現実に慣らすこと 誰もそんなことは お連れしました ことになりかねないので'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqoRE4e7KWJc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2RMYOGXKWDe",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# calculate BLEU score\n",
        "print('BLEU corpus: %f' % corpus_bleu(list(Y_train.values[6:-4]), predicted))\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# print(\"BLUE  sentense: %f\" % sentence_bleu(actual, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_tOYFO-kFuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}