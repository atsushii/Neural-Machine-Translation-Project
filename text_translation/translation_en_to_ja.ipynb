{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "translation_en_to_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjVpP_kv4yc",
        "colab_type": "code",
        "outputId": "25d7e779-a2ee-42d6-cb43-bcb0fd4df3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zb93R3DCCXK0",
        "outputId": "9273a990-f16a-42c1-e0f7-a8e110dc6824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import io\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# plot japanese lang\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/c0/b75d434be51a8cc11d2e9b36f2d7f93a1bcf63bde24dc79a61d329d60b2a/japanize-matplotlib-1.0.5.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 3.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.0.5-cp36-none-any.whl size=4118721 sha256=28228b4e7d294be9a42b36ff78e447453d306bec663bc92c92489dc132566b74\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/8a/08/4a784957da9f3c2b4839b4986be2fba2a481877318948be52c\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm1CEgijFK_B",
        "colab_type": "text"
      },
      "source": [
        "# load text file\n",
        "\n",
        "**this dataset is aleady implemented a SentenceSpace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmv0RqrACbd-",
        "colab": {}
      },
      "source": [
        "num_example = 50000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"/content/drive/My Drive/Colab Notebooks/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGytFt5qxDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl22ZyPk2PfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use other dataset\n",
        "en = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.en\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "ja = io.open(\"/content/drive/My Drive/OpenSubtitles2016.en-ja.ja\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "en = en[:30000]\n",
        "ja = ja[:30000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnsuu4-gzjB6",
        "colab_type": "code",
        "outputId": "f0fe7699-a457-422a-9584-f7e6c05ac7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# sentencepiece\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model. load(\"/content/drive/My Drive/wiki-ja.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 1.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtoY2Kl2Jai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # replace ▁ to ''\n",
        "  ja = [\" \".join(spm_model.encode_as_pieces(i)).replace(\"▁\", \"\") for i in ja]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhEAtSYFSys",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        "**Removing accented characters** \n",
        "\n",
        "e.g. é → e.\n",
        "\n",
        "**Expanding Contractions** \n",
        "\n",
        "e.g. don't → do not, I'd → I would\n",
        "\n",
        "**remove special word** \n",
        "\n",
        "e.g. remove \"123#@\"\n",
        "\n",
        "**Stemming**\n",
        "\n",
        " e.g. corder, codes → code\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        " e.g. better → good\n",
        "\n",
        "**Tokenize**\n",
        "\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS0AX_jOFk_l",
        "colab_type": "text"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ttcy6dKhDrmB",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OMHI0M1qPWqa",
        "outputId": "4519a296-85d2-4cf3-9183-15e132ab36e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('こんにちは。今日は', 'Hello world e ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO9i_n__FrHg",
        "colab_type": "text"
      },
      "source": [
        "# Expanding Contractions\n",
        "Japanese doesn't have a Contraction words so I just create a one function to expand Contractions for Engish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUEU_GU1PYNq",
        "colab": {}
      },
      "source": [
        "def expand_constraction(text):\n",
        "\n",
        "    #  dic for expand constraction words\n",
        "    constraction_dict= {\n",
        "        \"ain't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he he will have\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'd've\": \"I would have\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'd've\": \"i would have\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'll've\": \"i will have\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"needn't've\": \"need not have\",\n",
        "        \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"shan't've\": \"shall not have\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\",\n",
        "        \"so've\": \"so have\",\n",
        "        \"so's\": \"so as\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\",\n",
        "        \"there'd've\": \"there would have\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\",\n",
        "        \"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\n",
        "        \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'd've\": \"you would have\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    #  define match pattern\n",
        "    #  IGNORECASE → no matter if word is lowercase or uppercase\n",
        "    #  DOTAIL → . is going to match \\n\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(constraction_dict.keys())),\n",
        "                                                  flags=re.IGNORECASE | re.DOTALL)\n",
        "    #  expand words\n",
        "    def expand_match(constraction):\n",
        "        # get constraction word\n",
        "        match = constraction.group(0)\n",
        "        first_char = match[0]\n",
        "        #  get expand word from constraction dict\n",
        "        expand_constraction = constraction_dict.get(match)\\\n",
        "                                                    if constraction_dict.get(match) \\\n",
        "                                                    else constraction_dict.get(match.lower())\n",
        "        \n",
        "        # create expand constraction\n",
        "        expand_constraction = first_char + expand_constraction[1:]\n",
        "        return expand_constraction\n",
        "    \n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ikqxXGePadk",
        "outputId": "e17a6c9d-f3ec-44af-e135-33b7f2c9b836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "expand_constraction(\"you're good I'd like to go he's she's\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you are good I would like to go he is she is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZrYrRxFwc8",
        "colab_type": "text"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \"?\", \"!\", \"-\", \"ー\", \"Kanji\", \"Katakana\", \"Hiragana\") \n",
        "create space between word and punctuation (? ! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBMqrm3o-sJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space(text):\n",
        "    pattern = r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\-/\\s]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = re.sub(r\"([?!.,。、])\", r\" \\1 \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeOvXEYFQq_-",
        "outputId": "842d8189-aefb-4525-fec6-117b32f96beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello, . #@…123world.\"), replace_special_character_to_space(\"こん・にちは。・ いい天気。\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hello  world', 'こんにちは いい天気')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-z-PhGGQqL",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "I will do stemming only english which can create a base form of a word from a given word. Japanese language doesn't need a stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYiW2_Pl1lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemmer_word(text):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-iz13U2awVV",
        "colab_type": "code",
        "outputId": "149dec8a-013c-44a5-dc05-374dba7a11c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "stemmer_word(\"hello world she has cat but he had dogs he is went to traveling\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world she ha cat but he had dog he is went to travel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqW5RHMGifD",
        "colab_type": "text"
      },
      "source": [
        "# Normalize each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sizAmLv2Pj-j",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = ()\n",
        "    target_value = ()\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space(en_text)\n",
        "\n",
        "        # input value doesn't need  a START and END sentence  \n",
        "        input_value += (en_text, )\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = expand_constraction(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "\n",
        "        # add StTART and END sentence\n",
        "        ja_text = \"START_ \" + ja_text + \" _END\"\n",
        "        \n",
        "        target_value += (ja_text, )\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMa52HiWGtQ-",
        "colab_type": "text"
      },
      "source": [
        "# get clean text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1WLwCK4xVynu",
        "colab": {}
      },
      "source": [
        "# get normalize text data\n",
        "input_value, target_value = normalize_english(en, ja)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xAuLWUomimI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "18de27be-e7d2-41cc-f76a-210d5309abfe"
      },
      "source": [
        "pd.DataFrame({\"input\": x, \"target\": y}).head()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>you are back are not you harold ?</td>\n",
              "      <td>START_ あなたは戻ったのね ハロルド ?  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my opponent is shark</td>\n",
              "      <td>START_ 俺の相手は シャークだ _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is one thing in exchange for another</td>\n",
              "      <td>START_ 引き換えだ ある事とある物の _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yeah i am fine</td>\n",
              "      <td>START_ もういいよ ごちそうさま ううん _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do not come to the office anymore do not call ...</td>\n",
              "      <td>START_ もう会社には来ないでくれ 電話もするな _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input                            target\n",
              "0                 you are back are not you harold ?     START_ あなたは戻ったのね ハロルド ?  _END\n",
              "1                               my opponent is shark          START_ 俺の相手は シャークだ _END\n",
              "2          this is one thing in exchange for another       START_ 引き換えだ ある事とある物の _END\n",
              "3                                     yeah i am fine     START_ もういいよ ごちそうさま ううん _END\n",
              "4  do not come to the office anymore do not call ...  START_ もう会社には来ないでくれ 電話もするな _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLVeltmkUN1B",
        "colab_type": "code",
        "outputId": "2ed445d9-ab24-41e5-becb-a062a3fcffd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get unique vocab\n",
        "\n",
        "# input value \n",
        "all_eng = set()\n",
        "for input_lang in x:\n",
        "    for word in input_lang.split():\n",
        "        if word not in all_eng:\n",
        "            all_eng.add(word)\n",
        "\n",
        "# target value\n",
        "all_ja = set()\n",
        "for target_lang in y:\n",
        "    for word in target_lang.split():\n",
        "        if word not in all_ja:\n",
        "            all_ja.add(word)\n",
        "\n",
        "print(\"unique vocab %d\" % len(all_eng))\n",
        "print(\"unique vocab %d\" %len(all_ja))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique vocab 22249\n",
            "unique vocab 69148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5xXXCao7sm",
        "colab_type": "code",
        "outputId": "802c2173-bd06-4116-b7e2-f366449f69eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "# max length of input sentense and target sentense\n",
        "english_len = [len(i.split()) for i in x]\n",
        "\n",
        "japanese_len = [len(i.split()) for i in y]\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(english_len)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(japanese_len)\n",
        "\n",
        "# print max length\n",
        "print(\"english length:\", max(english_len))\n",
        "print(\"japanese length:\", max(japanese_len))\n",
        "max_len_input =  max(english_len)\n",
        "max_len_target =  max(japanese_len)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 44\n",
            "japanese length: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFPoVYpV8dP",
        "colab_type": "code",
        "outputId": "f64778f4-60ed-45e8-98eb-e0cda07a9e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_word = sorted(list(all_eng))\n",
        "target_word = sorted(list(all_ja))\n",
        "\n",
        "num_encoder_tokens = len(all_eng)\n",
        "num_decoder_tokens = len(all_ja)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22249, 69148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4zaIQbZa7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_decoder_tokens += 1\n",
        "num_encoder_tokens += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFCjZpvyWnpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([ (word, i+1) for i, word in enumerate(input_word)])\n",
        "target_token_index = dict([ (word, i+1) for i, word in enumerate(target_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiVPd5RXHHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXNxkRB9qoTU",
        "colab_type": "text"
      },
      "source": [
        "# split train data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdiAdGYR97Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WemPmLVUI7X8",
        "colab_type": "code",
        "outputId": "d4c89d58-6f1c-4380-bfa5-e08e81c5243d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24500,), (15000,), (10500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282RRGIkq01E",
        "colab_type": "text"
      },
      "source": [
        "# Create generater"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCwdO3dY-mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a batch of data\n",
        "def generate_batch(X = X_train, Y = Y_train, batch_size=64):\n",
        "    while True:\n",
        "        assert len(X) == len(Y)\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            # initialize input and target data\n",
        "            encoder_input_data = np.zeros((batch_size, max_len_input), dtype=\"float32\")\n",
        "            decoder_input_data = np.zeros((batch_size, max_len_target), dtype=\"float32\")\n",
        "            decoder_target_data = np.zeros((batch_size, max_len_target, num_decoder_tokens), dtype=\"float32\")\n",
        "            for j, (input_text, target_text) in enumerate(zip(X[i: i+batch_size], Y[i: i+batch_size])):\n",
        "                    # create enocder input dataset\n",
        "                    for t, word in enumerate(input_text.split()):\n",
        "                        encoder_input_data[j, t] = input_token_index[word]\n",
        "                    # create decoder input dataset\n",
        "                    for t, word in enumerate(target_text.split()):\n",
        "                        if t < len(target_text.split())-1:\n",
        "                            decoder_input_data[j, t] = target_token_index[word]\n",
        "                        # decoder  output is one-hot encoding\n",
        "                        # doesn't include START sentense\n",
        "                        # one timestep ahead of input \n",
        "                        if t > 0:\n",
        "                            decoder_target_data[j, t-1, target_token_index[word]] = 1.\n",
        "                    yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOzbWmLnZ66-",
        "colab_type": "text"
      },
      "source": [
        "# Define for params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYUC0fMZ6fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "units = 512\n",
        "lr = 0.001\n",
        "l2 = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Vtvs3hvt2Z",
        "colab_type": "text"
      },
      "source": [
        "# Build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215DTscueVam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define time step and dimention size for embedding and LSTM\n",
        "in_timesteps = max_len_input\n",
        "out_timesteps = max_len_target\n",
        "\n",
        "# build encoder model\n",
        "encoder_input = Input(shape=(in_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=units, mask_zero=True)(encoder_input) \n",
        "\n",
        "# Dropout\n",
        "encoder_embedding = Dropout(dropout)(encoder_embedding)\n",
        "\n",
        "# get enternal state to predict target value\n",
        "# encode output is going to use for attention\n",
        "encoder_output, state_h, state_c = LSTM(units, return_sequences=True, return_state=True, recurrent_regularizer=regularizers.l2(l=l2))(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "# it will use for test model\n",
        "test_encoder_state = [encoder_output, state_h, state_c ]\n",
        "\n",
        "# build decoder model\n",
        "decoder_input = Input(shape=(out_timesteps, ))\n",
        "\n",
        "# use pre-trained model Word2Vec\n",
        "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=units, mask_zero=True)\n",
        "decoder_emb = decoder_embedding(decoder_input)\n",
        "\n",
        "# Dropout\n",
        "decoder = Dropout(dropout)(decoder_emb)\n",
        "\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True, recurrent_regularizer=regularizers.l2(l=l2))\n",
        "# we don't use return state in the train model\n",
        "decoder, _, _ = decoder_lstm(decoder,  initial_state=encoder_states)\n",
        "\n",
        "# get attention weight\n",
        "t = Dense(5000, activation='relu')(decoder)\n",
        "t2 = Dense(5000, activation='relu')(encoder_output)\n",
        "attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "attention = Dense(in_timesteps, activation='relu')(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "\n",
        "context = dot([attention, encoder_output], axes = [2,1])\n",
        "\n",
        "decoder_combined_context = tensorflow.keras.layers.concatenate([context, decoder])\n",
        "\n",
        "decoder_combined_context=Dense(2000, activation='relu')(decoder_combined_context)\n",
        "output=(Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_combined_context)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXLs9SDRhIy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataset as generater\n",
        "gen_train = generate_batch(X_train, Y_train, batch_size)\n",
        "gen_val = generate_batch(X_val, Y_val, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6j01NDZj0Ce",
        "colab_type": "code",
        "outputId": "4e592dbf-c034-480d-f8d3-7a093e28d1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# caluclate train step\n",
        "from math import ceil\n",
        "trian_step = ceil(len(X_train) / batch_size )\n",
        "val_step = ceil(len(X_val) / batch_size)\n",
        "val_step"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opmqFdDziBng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO6Hd4RftoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = optimizers.SGD(lr=lr)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "# checkpoint\n",
        "filename = 'model.h1.22_Nov_19'\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', save_best_only=True, mode='min')              \n",
        "\n",
        "# train model\n",
        "history = model.fit_generator(gen_train, validation_data=gen_val , steps_per_epoch=trian_step, validation_steps=val_step, epochs=1, callbacks=[es], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55uxIDrTSvJ",
        "colab_type": "code",
        "outputId": "b8c37004-5912-4917-9691-bc27b83fa9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# define encoder model\n",
        "encoder_model = Model(encoder_input, test_encoder_state)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 44)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 44, 512)           11392512  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 44, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 44, 512), (None,  2099200   \n",
            "=================================================================\n",
            "Total params: 13,491,712\n",
            "Trainable params: 13,491,712\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4Rk1j6LwGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_decoder(encoder_out):\n",
        "    # define decoder architecuture\n",
        "    decoder_state_h = Input(shape=(units, ))\n",
        "    decoder_state_c = Input(shape=(units, ))\n",
        "    decoder_state = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "    # decoder embedding\n",
        "    dec_emb2 = decoder_embedding(decoder_input)\n",
        "    decoder2 = Dropout(0.1)(dec_emb2)\n",
        "\n",
        "    decoder_output, state_h2, state_c2 = decoder_lstm(decoder2, initial_state=decoder_state)\n",
        "\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "    t = Dense(5000, activation='relu')(decoder_output)\n",
        "    t2 = Dense(5000, activation='relu')(encoder_out)\n",
        "    attention = dot([t, t2], axes=[2, 2])\n",
        "\n",
        "    attention = Dense(in_timesteps, activation='relu')(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "\n",
        "    context = dot([attention, encoder_out], axes = [2,1])\n",
        "\n",
        "    decoder_combined_context = tensorflow.keras.layers.concatenate([context, decoder_output])\n",
        "\n",
        "    decoder_combined_context=Dense(2000, activation='relu')(decoder_combined_context)\n",
        "\n",
        "    decoder_output = (Dense(num_decoder_tokens, activation=\"softmax\"))(decoder_output)\n",
        "\n",
        "    decoder_model = Model([decoder_input] + decoder_state, [decoder_output] + decoder_states2)\n",
        "\n",
        "    return decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Zf2eAyTejb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_seq(input_seq):\n",
        "    # encoder the input seq as vector\n",
        "    output_en, a, b = encoder_model.predict(input_seq)\n",
        "    enc_output = encoder_model.layers[3].output[0]\n",
        "    en_state = [a, b]\n",
        "    # generate empty target sequence\n",
        "    target_seq = np.zeros((1, max_len_target))\n",
        "    # populate the first character of target seq\n",
        "    target_seq[0, 0] = target_token_index[\"START_\"]\n",
        "\n",
        "    # loop for batch of sequences\n",
        "    stop_condition = False\n",
        "    decoder_sentence = ''\n",
        "    decoder_model = define_decoder(enc_output)\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_token, h, c = decoder_model.predict([target_seq] + en_state)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_token[0, -1, :])\n",
        "        sampled_char = reverse_target_char[sampled_token_index]\n",
        "        decoder_sentence += ' ' + sampled_char\n",
        "\n",
        "\n",
        "        # stop condition\n",
        "        if sampled_char == \"_END\" or len(decoder_sentence) > max_len_input:\n",
        "          stop_condition = True\n",
        "        \n",
        "\n",
        "        # update the target sequence\n",
        "        target_seq = np.zeros((1, max_len_target))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # update states\n",
        "        en_state = [h, c]\n",
        "    \n",
        "    return decoder_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwXPOWqZIkik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_test, Y_test, 1)\n",
        "k = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZMb7fNtfLhc",
        "colab_type": "code",
        "outputId": "45811e2e-0af0-4485-b889-44a83e9eec83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoder_sentence = decoder_seq(input_seq)\n",
        "print(\"input english %s\" % X_test[k:k+1].values[0])\n",
        "print(\"actual %s\" % Y_test[k:k+1].values[0][6:-4])\n",
        "print(\"predict %s\" % decoder_sentence[:-4] )"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input english who is behind the camera flash ? \n",
            "actual  カメラの明かりだ 後ろに誰がいる ?  \n",
            "predict  言うの 飲み直しだよ どうなってしまうか もっと心臓に ヘリが到着しました ヘリが到着\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Lk0FA1TptP",
        "colab_type": "code",
        "outputId": "b82ca947-9bb6-42ef-c73f-ed22f6af1743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_sentence"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' その名前もきっとー 様な現実に慣らすこと 誰もそんなことは お連れしました ことになりかねないので'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqoRE4e7KWJc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2RMYOGXKWDe",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# calculate BLEU score\n",
        "print('BLEU corpus: %f' % corpus_bleu(list(Y_train.values[6:-4]), predicted))\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# print(\"BLUE  sentense: %f\" % sentence_bleu(actual, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_tOYFO-kFuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}