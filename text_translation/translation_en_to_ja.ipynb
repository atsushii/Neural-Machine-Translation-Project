{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "translation_en_to_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jK88JoQ79a_G",
        "outputId": "915e7861-8684-4b8b-84df-0716a8e336a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install mojimoji\n",
        "!pip install sentencepiece\n",
        "import mojimoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "import nltk\n",
        "import unicodedata\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(tf.__version__)\n",
        "# gpu\n",
        "tf.test.is_gpu_available() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mojimoji in /usr/local/lib/python3.6/dist-packages (0.0.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J_L_ygdR9a_Q"
      },
      "source": [
        "# load text file\n",
        "\n",
        "this dataset is aleady implemented a SentenceSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jYlzMp1U9a_T",
        "colab": {}
      },
      "source": [
        "num_example = 300000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qG4dRId39a_X",
        "colab": {}
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrWjSfDlABKb",
        "colab_type": "text"
      },
      "source": [
        "**SentencePiece**\n",
        "\n",
        "using pretrain model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5WysbRO9-nj",
        "colab": {}
      },
      "source": [
        "ja_sentence = list()\n",
        "for i in ja:\n",
        "    ja_sentence.append(i.replace(\" \", \"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J9M9slUl9_hh",
        "colab": {}
      },
      "source": [
        "ja_text = list()\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"wiki-ja.model\")\n",
        "for text in ja_sentence:\n",
        "    ja_text.append(\" \".join(sp.EncodeAsPieces(text)).replace(\"▁\", \"\").strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "os6q5rD-9a_d"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        " **Removing accented characters**\n",
        " e.g. é → e.\n",
        " \n",
        "\n",
        "**remove special word**\n",
        "e.g. remove \"123#@!\"\n",
        "\n",
        "\n",
        "**normalize japanese**\n",
        "e.g.  カナダ  → ｶﾅﾀﾞ\n",
        "\n",
        "**Tokenize**\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZBfMYKbAR5w",
        "colab_type": "text"
      },
      "source": [
        "# normalize Japanese (Hiragana and Katakana)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tD17_NhQ-Dph",
        "colab": {}
      },
      "source": [
        "def moji(text):\n",
        "  return mojimoji.zen_to_han(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN4FzAjRAhLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af19ca4a-133a-43e6-8b97-fd3ee8aef4cf"
      },
      "source": [
        "print(\"カナダ →\", moji(\"カナダ\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "カナダ → ｶﾅﾀﾞ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IlFoNcZK9a_d"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent\n",
        "I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zBNtprYP9a_e",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fed-yYrU9a_h",
        "outputId": "b11dad69-de38-495a-fdd7-27d49c8ea821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。 今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('こんにちは。 今日は', 'Hello world e ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ogJT2Kr9a_x"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
        "create space between word and punctuation (? ! . , 、 。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yiKfZSxd9a_x",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space_en(text):\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "  \n",
        "def replace_special_character_to_space(text):\n",
        "    text = re.sub(r\"([?!。、¿])\", r\" \\1\", text)\n",
        "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\s、。.,0-9]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ifLQFqOU9a_2",
        "outputId": "3d2040da-bd55-43b5-dca6-f1a444f0f34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello #@…123world.\"), replace_special_character_to_space(\"こん・にちは・いい天気。\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('123.', 'こんにちはいい天気 。')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txwH8f9X9bAL"
      },
      "source": [
        "# Text normalize\n",
        "\n",
        "I will normalize English text and Japanese text using function which is defined so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WLgyXVAy9bAN",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = []\n",
        "    target_value = []\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "        # input value doesn't need  a START and END sentence  \n",
        "        input_value.append(en_text)\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "        ja_text = moji(ja_text)\n",
        "\n",
        "        # add StTART and END sentence\n",
        "        ja_text = \"start_ \" + ja_text + \" _end\"\n",
        "        \n",
        "        target_value.append(ja_text)\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fv1iASn49bAR",
        "colab": {}
      },
      "source": [
        "# get normalize text data\n",
        "input_value, target_value = normalize_english(en, ja_text)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2-4jMOfx9bAW",
        "outputId": "3d804cbc-ef27-49bf-bb44-4f537697e32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "pd.DataFrame({\"input\": x, \"target\": y}).head(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>start_ you are back , aren t you , harold ? _end</td>\n",
              "      <td>start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start_ my opponent is shark . _end</td>\n",
              "      <td>start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>start_ this is one thing in exchange for anoth...</td>\n",
              "      <td>start_ 引き 換え だ ある 事 とある 物の _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>start_ yeah , i m fine . _end</td>\n",
              "      <td>start_ もう いい よ ご ち そう さま う うん _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>start_ don t come to the office anymore . don ...</td>\n",
              "      <td>start_ もう 会社 には 来 ない で くれ 電話 も する な _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>start_ looks beautiful . _end</td>\n",
              "      <td>start_ きれい だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>start_ get him out of here , because i will fu...</td>\n",
              "      <td>start_ 連れ て 行 け 殺し そう だ わ かった か ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>start_ you killed him ! _end</td>\n",
              "      <td>start_ 殺 した のか ! _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>start_ okay , then who ? _end</td>\n",
              "      <td>start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>start_ it seems a former employee . . . _end</td>\n",
              "      <td>start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input                                     target\n",
              "0   start_ you are back , aren t you , harold ? _end          start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end\n",
              "1                 start_ my opponent is shark . _end            start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end\n",
              "2  start_ this is one thing in exchange for anoth...           start_ 引き 換え だ ある 事 とある 物の _end\n",
              "3                      start_ yeah , i m fine . _end        start_ もう いい よ ご ち そう さま う うん _end\n",
              "4  start_ don t come to the office anymore . don ...  start_ もう 会社 には 来 ない で くれ 電話 も する な _end\n",
              "5                      start_ looks beautiful . _end                       start_ きれい だ ｡ _end\n",
              "6  start_ get him out of here , because i will fu...    start_ 連れ て 行 け 殺し そう だ わ かった か ? _end\n",
              "7                       start_ you killed him ! _end                      start_ 殺 した のか ! _end\n",
              "8                      start_ okay , then who ? _end    start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end\n",
              "9       start_ it seems a former employee . . . _end                start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZMlaIsar9bAf"
      },
      "source": [
        "# tokenize\n",
        "tokenize each language word based on space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ACN_sW7u9bAh",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JCZ7huXZ9bAs",
        "outputId": "6189376e-b127-4b80-85cf-f7d1162e4345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# e.g.\n",
        "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold\"])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  1,  4],\n",
              "        [ 5,  6,  7,  8],\n",
              "        [ 9,  1, 10, 11]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7fe876dddcc0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KDfQw_5t9bA5"
      },
      "source": [
        "# create clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sIM8-10h9bA6",
        "colab": {}
      },
      "source": [
        "# cleate a clean dataset\n",
        "def create_dataset(en, ja):\n",
        "    \n",
        "    # input_tensor, target_tensor: 2d numpy array\n",
        "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
        "    input_tensor, input_lang_tokenize = tokenize(en)\n",
        "    target_tensor, target_lang_tokenize = tokenize(ja)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jEJJ9BMgATdD",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lzlBqTlO9bA-",
        "colab": {}
      },
      "source": [
        "def max_length(input_tensor, target_tensor):\n",
        "\n",
        "    # max length of input sentense and target sentense\n",
        "    english_len = [len(i) for i in input_tensor]\n",
        "\n",
        "    japanese_len = [len(i) for i in target_tensor]\n",
        "\n",
        "     # print max length\n",
        "    print(\"english length:\", max(english_len))\n",
        "    print(\"japanese length:\", max(japanese_len))\n",
        "    max_len_input =  max(english_len)\n",
        "    max_len_target =  max(japanese_len)\n",
        "\n",
        "    return max_len_input, max_len_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ew-q24gJ9bBB",
        "outputId": "1939ab42-fd2c-463e-ff9a-27ec5356d451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_input, max_length_target = max_length(input_tensor, target_tensor)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 69\n",
            "japanese length: 86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y_HvHj059bBG",
        "outputId": "29defb60-5136-49f5-b8f7-4fed80f5fe8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create trainnig set and validation set\n",
        "X_train, X_test, \\\n",
        "    Y_train, Y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_test, X_val, \\\n",
        "    Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, shuffle=True)\n",
        "\n",
        "\n",
        "# show length\n",
        "print(len(X_train), len(Y_train), len(X_test), len(Y_test), len(X_val), len(Y_val))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240000 240000 30000 30000 30000 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w12Za_JK9bBJ",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6yT2odT9bBP",
        "outputId": "a0315d4d-13f0-4f54-9c22-e11d2642c993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang_tokenize, X_train[10])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang_tokenize, Y_train[10])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1----->start_\n",
            "172----->call\n",
            "6----->the\n",
            "301----->car\n",
            "2----->_end\n",
            "output lang: index to word mapping\n",
            "1----->start_\n",
            "2040----->車を\n",
            "1551----->呼\n",
            "619----->べ\n",
            "2----->_end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSHdoFwQ9bBT"
      },
      "source": [
        "# define parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jhj29n1l9bBT",
        "outputId": "ed8d71bc-0a88-4af1-c0b6-64bd45d2ecae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 32\n",
        "dropout_rate = 0.3\n",
        "# if None steps_per_epoch == mum of dataset\n",
        "train_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "val_steps_per_epoch = len(X_val) // BATCH_SIZE\n",
        "print(\"train step %d\" % train_steps_per_epoch)\n",
        "embedding_dim = 300\n",
        "units = 512\n",
        "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang_tokenize.word_index))\n",
        "print('Total unique words in the target: %s' % len(target_lang_tokenize.word_index))\n",
        "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
        "\n",
        "# create train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step 7500\n",
            "Total unique words in the input: 52080\n",
            "Total unique words in the target: 25498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjAVocfI9bCI"
      },
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJWSAHhl9bCJ",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    return_state=True,\n",
        "                                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.first_lstm(x, initial_state =hidden)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c ]\n",
        "\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53dyDZO_F35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZSn1VowJ9bCS"
      },
      "source": [
        "# attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cctmI1mA9bCS",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, units: int, *args, **kwargs):\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.units = units\n",
        "\n",
        "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
        "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
        "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
        "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
        "\n",
        "    def call(self, input, memory):\n",
        "\n",
        "        q = self.q_dense_layer(input) \n",
        "        k = self.k_dense_layer(memory) \n",
        "        v = self.v_dense_layer(memory)\n",
        "\n",
        "        depth = self.units // 2\n",
        "        q *= depth ** -0.5  # for scaled dot production\n",
        "\n",
        "        # caluclate relation between query and key\n",
        "        logit = tf.matmul(q, k, transpose_b=True) \n",
        "\n",
        "        attention_weight = tf.nn.softmax(logit)\n",
        "\n",
        "        attention_output = tf.matmul(attention_weight, v) \n",
        "        return self.output_dense_layer(attention_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBBPzDxK9bCg"
      },
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1nED5gCZ9bCh",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True)\n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True)\n",
        "                                                            \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x =  self.first_lstm(x)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c]\n",
        "        attention_weights = self.attention(output, enc_output)\n",
        "        output = tf.concat([output, attention_weights], axis=-1)\n",
        "\n",
        "                \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        output = self.fc(output)\n",
        "        \n",
        "        return  output, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-lrVTN_F4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvyXf1bn9bC1"
      },
      "source": [
        "# optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wROZueWU9bC3",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DtDLuqB49bC7"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0CWEvq3W9bC8",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './train_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ja0mhQ4j9bDB"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQSV9kkd9bDB",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the sladecoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rCI9q4-C9bDI",
        "outputId": "549c58c0-f50a-488e-c7ec-c9c8616709e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# trained model for 45 epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(train_steps_per_epoch)):\n",
        "    train_batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss += train_batch_loss\n",
        "\n",
        "\n",
        "  for (batch, (val_inp, val_tar)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
        "    val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
        "    val_loss += val_batch_loss\n",
        "\n",
        "\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,\n",
        "                                      train_loss / train_steps_per_epoch))\n",
        "  print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                      val_loss / val_steps_per_epoch))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss 0.3737\n",
            "Epoch 1 Validation Loss 0.3688\n",
            "Epoch 2 Train Loss 0.3702\n",
            "Epoch 2 Validation Loss 0.3662\n",
            "Epoch 3 Train Loss 0.3672\n",
            "Epoch 3 Validation Loss 0.3622\n",
            "Epoch 4 Train Loss 0.3681\n",
            "Epoch 4 Validation Loss 0.3660\n",
            "Epoch 5 Train Loss 0.3711\n",
            "Epoch 5 Validation Loss 0.3687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOhXwpZlDwJp",
        "colab_type": "text"
      },
      "source": [
        "# Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVDzzIKJBsBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b01509c-c255-4c49-96ae-e38af7678bb1"
      },
      "source": [
        "checkpoint.restore(\"train_model/ckpt-5\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe795e7cda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQnK48R0BuZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# into base model\n",
        "encoder = checkpoint.encoder\n",
        "decoder = checkpoint.decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gu5bPpdEHVz",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Check bleu score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ8r9JNREs01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "    inputs = tf.convert_to_tensor(sentence)\n",
        "    result = ''\n",
        "    inputs = tf.expand_dims(inputs, axis=0)\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87pBzvXjRkJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_reference(lang, tensor):\n",
        "    all_sentence_list = []\n",
        "\n",
        "    for word_list in tensor:\n",
        "      sentence_list = []\n",
        "\n",
        "      for t in word_list:\n",
        "          if not t == 0:\n",
        "              # Index number assigned to each word\n",
        "              sentence_list.append(lang.index_word[t])\n",
        "      all_sentence_list.append(sentence_list)\n",
        "    return all_sentence_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwwlmHtRrfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create reference\n",
        "reference = create_reference(target_lang_tokenize, Y_test.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAMX-vQEKB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca4b91bb-c8f5-4a74-bc38-8722c206b50e"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# create hypothesis\n",
        "hypothesis = []\n",
        "for i in tqdm(X_test):\n",
        "  hypothesis.append(predict(i))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [52:33<00:00,  9.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0Jl_21Ee7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21c06b14-9ad2-4c86-8c20-db42bfd5af9a"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "score = 0\n",
        "smoothie = SmoothingFunction().method2\n",
        "for i in range(len(reference)):\n",
        "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
        "\n",
        "score /= len(reference)\n",
        "print(\"The bleu score is: \"+str(score))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bleu score is: 0.12005058946101176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNkXiemq9bDO"
      },
      "source": [
        "# Translation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J-DcisWA9bDP",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(en_text):\n",
        "        # normalize English\n",
        "        en_text = en_text.lower()\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "\n",
        "        return en_text\n",
        "        \n",
        "def evaluate(sentence):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ui6bFxRt9bDd",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def result(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2duMXKX9bDh",
        "outputId": "91e37647-4a87-4413-a5cb-c1835bab223d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result, sentence = result(\"he has a dog\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ he has a dog _end\n",
            "Predicted translation: 犬 が _end \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XlCu-uvtwOft",
        "outputId": "5f49c5eb-a7bb-4ff4-f3fb-f880a27dfb9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result, sentence = result(\"What is this!?\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ what is this ! ? _end\n",
            "Predicted translation: これは 何 だよ な に _end \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2FZwaiky-yj",
        "outputId": "f4d68bbf-134a-43b7-e54f-f0fd9ff2c07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result, sentence = result(\"Actually I did it\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ actually i did it _end\n",
            "Predicted translation: や った _end \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67d09FoPz2Al",
        "outputId": "6fc6029c-d40f-4ef5-b3dc-6675c57aac1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result, sentence = result(\"I love her\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ i love her _end\n",
            "Predicted translation: 彼女 を愛し て _end \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J2saSpKK0Cwl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}