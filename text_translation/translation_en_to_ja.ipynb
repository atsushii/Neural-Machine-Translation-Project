{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "translation_en_to_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFb62Dmn9r9i",
        "colab_type": "code",
        "outputId": "d217c9dc-8447-4115-a850-104f6759eb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK88JoQ79a_G",
        "colab_type": "code",
        "outputId": "e5f4fe66-46b8-4824-b7fa-30a68bd81085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "!pip install mojimoji\n",
        "import mojimoji\n",
        "\n",
        "!pip install sentencepiece\n",
        "\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Bidirectional, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import io\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import unicodedata\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(tf.__version__)\n",
        "# gpu\n",
        "tf.test.is_gpu_available() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mojimoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/24/21fff7ab1d69b20e32ae75742bea606df7d88ddf46d05880318e33b8cf57/mojimoji-0.0.10-cp36-cp36m-manylinux1_x86_64.whl (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: mojimoji\n",
            "Successfully installed mojimoji-0.0.10\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "2.2.0-rc2\n",
            "WARNING:tensorflow:From <ipython-input-2-9bdeccb86386>:34: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_L_ygdR9a_Q",
        "colab_type": "text"
      },
      "source": [
        "# load text file\n",
        "\n",
        "this dataset is aleady implemented a SentenceSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlzMp1U9a_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_example = 50000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"/content/drive/My Drive/Colab Notebooks/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG4dRId39a_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5WysbRO9-nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ja_sentence = list()\n",
        "for i in ja:\n",
        "    ja_sentence.append(i.replace(\" \", \"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9M9slUl9_hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ja_text = list()\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"/content/drive/My Drive/wiki-ja.model\")\n",
        "for text in ja_sentence:\n",
        "    ja_text.append(\" \".join(sp.EncodeAsPieces(text)).replace(\"▁\", \"\").strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os6q5rD-9a_d",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        " **Removing accented characters**\n",
        " e.g. é → e.\n",
        " \n",
        " **Expanding Contractions**\n",
        "e.g. don't → do not, I'd → I would\n",
        "\n",
        "\n",
        "**remove special word**\n",
        "e.g. remove \"123#@!\"\n",
        "\n",
        "\n",
        "**Stemming**\n",
        "e.g. corder, codes → code\n",
        "\n",
        "\n",
        "**Lemmatization**\n",
        "e.g. better → good\n",
        "\n",
        "**normalize japanese**\n",
        "e.g.  カナダ  → ｶﾅﾀﾞ\n",
        "\n",
        "**Tokenize**\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD17_NhQ-Dph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moji(text):\n",
        "  return mojimoji.zen_to_han(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFoNcZK9a_d",
        "colab_type": "text"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent\n",
        "I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBNtprYP9a_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fed-yYrU9a_h",
        "colab_type": "code",
        "outputId": "db767203-6f63-46b4-b646-50ec86e22a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。 今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('こんにちは。 今日は', 'Hello world e ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEKZ20sC9a_l",
        "colab_type": "text"
      },
      "source": [
        "# Expanding Contractions\n",
        "\n",
        "Japanese doesn't have a Contraction words so I just create a one function to expand Contractions for Engish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogJT2Kr9a_x",
        "colab_type": "text"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
        "create space between word and punctuation (? ! . , 、 。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiKfZSxd9a_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space_en(text):\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "  \n",
        "def replace_special_character_to_space(text):\n",
        "    text = re.sub(r\"([?!。、¿])\", r\" \\1\", text)\n",
        "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\s、。.,0-9]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifLQFqOU9a_2",
        "colab_type": "code",
        "outputId": "aa7a8afa-aea3-4a5d-a614-79e588267a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello #@…123world.\"), replace_special_character_to_space(\"こん・にちは・いい天気。\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('123.', 'こんにちはいい天気 。')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpYH1Mlx9a_5",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "\n",
        "I will do stemming only english which can create a base form of a word from a given word.\n",
        "Japanese language doesn't need a stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmk7pnfG9a_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemmer_word(text):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFEsQhoi9bAE",
        "colab_type": "code",
        "outputId": "614005df-a66b-4345-e09b-e3d3dbbf704c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# e.g.\n",
        "stemmer_word(\"hello world she has cat but he had dogs he is went to traveling\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world she ha cat but he had dog he is went to travel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwH8f9X9bAL",
        "colab_type": "text"
      },
      "source": [
        "# Text normalize\n",
        "\n",
        "I will normalize English text and Japanese text using function which is defined so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLgyXVAy9bAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = []\n",
        "    target_value = []\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "        # input value doesn't need  a START and END sentence  \n",
        "        input_value.append(en_text)\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "        ja_text = moji(ja_text)\n",
        "\n",
        "        # add StTART and END sentence\n",
        "        ja_text = \"start_ \" + ja_text + \" _end\"\n",
        "        \n",
        "        target_value.append(ja_text)\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv1iASn49bAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get normalize text data\n",
        "input_value, target_value = normalize_english(en, ja_text)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-4jMOfx9bAW",
        "colab_type": "code",
        "outputId": "9304fdbf-b6ab-431d-aa88-5f8cabdd9fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "pd.DataFrame({\"input\": x, \"target\": y}).head(20)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>start_ you are back , aren t you , harold ? _end</td>\n",
              "      <td>start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start_ my opponent is shark . _end</td>\n",
              "      <td>start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>start_ this is one thing in exchange for anoth...</td>\n",
              "      <td>start_ 引き 換え だ ある 事 とある 物の _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>start_ yeah , i m fine . _end</td>\n",
              "      <td>start_ もう いい よ ご ち そう さま う うん _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>start_ don t come to the office anymore . don ...</td>\n",
              "      <td>start_ もう 会社 には 来 ない で くれ 電話 も する な _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>start_ looks beautiful . _end</td>\n",
              "      <td>start_ きれい だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>start_ get him out of here , because i will fu...</td>\n",
              "      <td>start_ 連れ て 行 け 殺し そう だ わ かった か ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>start_ you killed him ! _end</td>\n",
              "      <td>start_ 殺 した のか ! _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>start_ okay , then who ? _end</td>\n",
              "      <td>start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>start_ it seems a former employee . . . _end</td>\n",
              "      <td>start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>start_ so where are they ? _end</td>\n",
              "      <td>start_ じゃ ｱｲ ﾂ ﾗ は どこ ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>start_ or i don t know , just . . . _end</td>\n",
              "      <td>start_ 相手を 陥 れる とか ... _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>start_ no child should ever have to endure tha...</td>\n",
              "      <td>start_ 必要 のない 子 耐え る に している ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>start_ i know the visual s incongruous , but ....</td>\n",
              "      <td>start_ 犯人 像 とは 違和感 のある 見た目 だが _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>start_ aw ! _end</td>\n",
              "      <td>start_ あ ! _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>start_ let me show you more detailed ones . _end</td>\n",
              "      <td>start_ もっと 詳しい の 見せ ます から ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>start_ you re gonna bring that up right now ? ...</td>\n",
              "      <td>start_ それ 今 持ち 出す か ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>start_ well then , when you re done work , com...</td>\n",
              "      <td>start_ じゃ あ 仕事 終わった ら 医 局 に 来て よ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>start_ um . . . please go ahead and eat withou...</td>\n",
              "      <td>start_ あの ... 皆 さん で 先 済 ませ て ください ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>start_ rokka is always forever _end</td>\n",
              "      <td>start_ 六 花 ちゃん は ずっと ずっと ずっと _end</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                input                                       target\n",
              "0    start_ you are back , aren t you , harold ? _end            start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end\n",
              "1                  start_ my opponent is shark . _end              start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end\n",
              "2   start_ this is one thing in exchange for anoth...             start_ 引き 換え だ ある 事 とある 物の _end\n",
              "3                       start_ yeah , i m fine . _end          start_ もう いい よ ご ち そう さま う うん _end\n",
              "4   start_ don t come to the office anymore . don ...    start_ もう 会社 には 来 ない で くれ 電話 も する な _end\n",
              "5                       start_ looks beautiful . _end                         start_ きれい だ ｡ _end\n",
              "6   start_ get him out of here , because i will fu...      start_ 連れ て 行 け 殺し そう だ わ かった か ? _end\n",
              "7                        start_ you killed him ! _end                        start_ 殺 した のか ! _end\n",
              "8                       start_ okay , then who ? _end      start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end\n",
              "9        start_ it seems a former employee . . . _end                  start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end\n",
              "10                    start_ so where are they ? _end               start_ じゃ ｱｲ ﾂ ﾗ は どこ ? _end\n",
              "11           start_ or i don t know , just . . . _end                  start_ 相手を 陥 れる とか ... _end\n",
              "12  start_ no child should ever have to endure tha...           start_ 必要 のない 子 耐え る に している ｡ _end\n",
              "13  start_ i know the visual s incongruous , but ....         start_ 犯人 像 とは 違和感 のある 見た目 だが _end\n",
              "14                                   start_ aw ! _end                              start_ あ ! _end\n",
              "15   start_ let me show you more detailed ones . _end             start_ もっと 詳しい の 見せ ます から ｡ _end\n",
              "16  start_ you re gonna bring that up right now ? ...                   start_ それ 今 持ち 出す か ? _end\n",
              "17  start_ well then , when you re done work , com...     start_ じゃ あ 仕事 終わった ら 医 局 に 来て よ ｡ _end\n",
              "18  start_ um . . . please go ahead and eat withou...  start_ あの ... 皆 さん で 先 済 ませ て ください ｡ _end\n",
              "19                start_ rokka is always forever _end         start_ 六 花 ちゃん は ずっと ずっと ずっと _end"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMlaIsar9bAf",
        "colab_type": "text"
      },
      "source": [
        "# tokenize\n",
        "tokenize each language word based on space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACN_sW7u9bAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCZ7huXZ9bAs",
        "colab_type": "code",
        "outputId": "59be28b6-d0ab-4e22-f2d4-d2fd8fc6dda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# e.g.\n",
        "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold\"])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  1,  4],\n",
              "        [ 5,  6,  7,  8],\n",
              "        [ 9,  1, 10, 11]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7f408187a860>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDfQw_5t9bA5",
        "colab_type": "text"
      },
      "source": [
        "# create clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIM8-10h9bA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleate a clean dataset\n",
        "def create_dataset(en, ja):\n",
        "    \n",
        "    # input_tensor, target_tensor: 2d numpy array\n",
        "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
        "    input_tensor, input_lang_tokenize = tokenize(en)\n",
        "    target_tensor, target_lang_tokenize = tokenize(ja)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJJ9BMgATdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAOQpMQbAXJ0",
        "colab_type": "code",
        "outputId": "89f312cc-c83f-48d1-f7dd-55069cbd5c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_lang_tokenize"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f4131a9a278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzlBqTlO9bA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(input_tensor, target_tensor):\n",
        "\n",
        "\n",
        "    # max length of input sentense and target sentense\n",
        "    english_len = [len(i) for i in input_tensor]\n",
        "\n",
        "    japanese_len = [len(i) for i in target_tensor]\n",
        "\n",
        " \n",
        "    # print max length\n",
        "    print(\"english length:\", max(english_len))\n",
        "    print(\"japanese length:\", max(japanese_len))\n",
        "    max_len_input =  max(english_len)\n",
        "    max_len_target =  max(japanese_len)\n",
        "\n",
        "    return max_len_input, max_len_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew-q24gJ9bBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "56edae70-2cc1-4b21-841c-3bc0ffb6c571"
      },
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_input, max_length_target = max_length(input_tensor, target_tensor)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 66\n",
            "japanese length: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HvHj059bBG",
        "colab_type": "code",
        "outputId": "aa032d34-a184-40ca-c298-5a063444b6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create trainnig set and validation set\n",
        "input_tensor_train, input_tensor_val, \\\n",
        "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# show length\n",
        "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000 10000 40000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w12Za_JK9bBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6yT2odT9bBP",
        "colab_type": "code",
        "outputId": "bb2063e8-d437-4379-cb8f-c8415f1a6829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang_tokenize, input_tensor_train[10])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang_tokenize, target_tensor_train[10])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1----->start_\n",
            "2871----->serial\n",
            "4192----->killers\n",
            "3----->.\n",
            "2----->_end\n",
            "output lang: index to word mapping\n",
            "1----->start_\n",
            "931----->研究\n",
            "3177----->分野\n",
            "4----->は\n",
            "7754----->ｼﾘｱﾙ\n",
            "4362----->ｷﾗｰ\n",
            "10----->だ\n",
            "2----->_end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSHdoFwQ9bBT",
        "colab_type": "text"
      },
      "source": [
        "# define parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhj29n1l9bBT",
        "colab_type": "code",
        "outputId": "8a1568c0-0961-455c-b206-b4b534710615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "dropout_rate = 0.3\n",
        "# if None steps_per_epoch == mum of dataset\n",
        "train_steps_per_epoch = len(input_tensor_train)\n",
        "val_steps_per_epoch = len(input_tensor_val)\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang_tokenize.word_index))\n",
        "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
        "\n",
        "\n",
        "# create train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unique words in the input: 21576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkeZF95T9bBf",
        "colab_type": "code",
        "outputId": "c92d8ec9-af70-433a-81ca-c39d132c89bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch =  next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 66]), TensorShape([64, 43]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCT0fBiv9bBu",
        "colab_type": "text"
      },
      "source": [
        "**explore japanese Word2Vec pre-train model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjAVocfI9bCI",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJWSAHhl9bCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    return_state=True,\n",
        "                                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.first_lstm(x, initial_state =hidden)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c ]\n",
        "\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSn1VowJ9bCS",
        "colab_type": "text"
      },
      "source": [
        "# attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cctmI1mA9bCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleAttention(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, units: int, *args, **kwargs):\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.units = units\n",
        "\n",
        "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
        "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
        "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
        "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
        "\n",
        "    def call(self, input, memory):\n",
        "\n",
        "        q = self.q_dense_layer(input) \n",
        "        k = self.k_dense_layer(memory) \n",
        "        v = self.v_dense_layer(memory)\n",
        "\n",
        "        depth = self.units // 2\n",
        "        q *= depth ** -0.5  # for scaled dot production\n",
        "\n",
        "        # caluclate relation between query and key\n",
        "        logit = tf.matmul(q, k, transpose_b=True) \n",
        "\n",
        "        attention_weight = tf.nn.softmax(logit)\n",
        "\n",
        "        attention_output = tf.matmul(attention_weight, v) \n",
        "        return self.output_dense_layer(attention_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBBPzDxK9bCg",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nED5gCZ9bCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, target_embedding_matrix, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True)\n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True)\n",
        "                                                            \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = SimpleAttention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x =  self.first_lstm(x)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c]\n",
        "        attention_weights = self.attention(output, enc_output)\n",
        "        output = tf.concat([output, attention_weights], axis=-1)\n",
        "\n",
        "                \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        output = self.fc(output)\n",
        "        \n",
        "        return  output, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvyXf1bn9bC1",
        "colab_type": "text"
      },
      "source": [
        "# optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wROZueWU9bC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDLuqB49bC7",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CWEvq3W9bC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja0mhQ4j9bDB",
        "colab_type": "text"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQSV9kkd9bDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCI9q4-C9bDI",
        "colab_type": "code",
        "outputId": "549c58c0-f50a-488e-c7ec-c9c8616709e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(train_steps_per_epoch)):\n",
        "    train_batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss += train_batch_loss\n",
        "\n",
        "    if batch % 200 == 0:\n",
        "      print('Train datta Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   train_batch_loss.numpy()))\n",
        "\n",
        "  for (batch, (val_inp, val_tar)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
        "    val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
        "    val_loss += val_batch_loss\n",
        "\n",
        "    if batch % 200 == 0:\n",
        "      print('validation datta Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   val_batch_loss.numpy()))\n",
        "\n",
        "\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  # if (epoch + 1) % 2 == 0:\n",
        "  #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      train_loss / train_steps_per_epoch))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      val_loss / val_steps_per_epoch))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train datta Epoch 1 Batch 0 Loss 1.0631\n",
            "Train datta Epoch 1 Batch 200 Loss 1.1212\n",
            "Train datta Epoch 1 Batch 400 Loss 1.0413\n",
            "Train datta Epoch 1 Batch 600 Loss 1.1313\n",
            "validation datta Epoch 1 Batch 0 Loss 1.1407\n",
            "Epoch 1 Loss 0.0179\n",
            "Epoch 1 Loss 0.0175\n",
            "Train datta Epoch 2 Batch 0 Loss 1.0392\n",
            "Train datta Epoch 2 Batch 200 Loss 1.1046\n",
            "Train datta Epoch 2 Batch 400 Loss 1.1102\n",
            "Train datta Epoch 2 Batch 600 Loss 1.0447\n",
            "validation datta Epoch 2 Batch 0 Loss 1.1103\n",
            "Epoch 2 Loss 0.0172\n",
            "Epoch 2 Loss 0.0167\n",
            "Train datta Epoch 3 Batch 0 Loss 1.0580\n",
            "Train datta Epoch 3 Batch 200 Loss 1.1142\n",
            "Train datta Epoch 3 Batch 400 Loss 1.0408\n",
            "Train datta Epoch 3 Batch 600 Loss 1.0748\n",
            "validation datta Epoch 3 Batch 0 Loss 1.0770\n",
            "Epoch 3 Loss 0.0166\n",
            "Epoch 3 Loss 0.0161\n",
            "Train datta Epoch 4 Batch 0 Loss 1.0393\n",
            "Train datta Epoch 4 Batch 200 Loss 1.1169\n",
            "Train datta Epoch 4 Batch 400 Loss 1.1401\n",
            "Train datta Epoch 4 Batch 600 Loss 1.0088\n",
            "validation datta Epoch 4 Batch 0 Loss 1.0497\n",
            "Epoch 4 Loss 0.0161\n",
            "Epoch 4 Loss 0.0155\n",
            "Train datta Epoch 5 Batch 0 Loss 1.1759\n",
            "Train datta Epoch 5 Batch 200 Loss 0.9942\n",
            "Train datta Epoch 5 Batch 400 Loss 0.9095\n",
            "Train datta Epoch 5 Batch 600 Loss 1.0372\n",
            "validation datta Epoch 5 Batch 0 Loss 1.0156\n",
            "Epoch 5 Loss 0.0156\n",
            "Epoch 5 Loss 0.0150\n",
            "Train datta Epoch 6 Batch 0 Loss 1.2137\n",
            "Train datta Epoch 6 Batch 200 Loss 0.8757\n",
            "Train datta Epoch 6 Batch 400 Loss 0.9762\n",
            "Train datta Epoch 6 Batch 600 Loss 1.0054\n",
            "validation datta Epoch 6 Batch 0 Loss 0.9827\n",
            "Epoch 6 Loss 0.0151\n",
            "Epoch 6 Loss 0.0145\n",
            "Train datta Epoch 7 Batch 0 Loss 0.9009\n",
            "Train datta Epoch 7 Batch 200 Loss 0.9547\n",
            "Train datta Epoch 7 Batch 400 Loss 0.9152\n",
            "Train datta Epoch 7 Batch 600 Loss 0.8661\n",
            "validation datta Epoch 7 Batch 0 Loss 0.9443\n",
            "Epoch 7 Loss 0.0147\n",
            "Epoch 7 Loss 0.0141\n",
            "Train datta Epoch 8 Batch 0 Loss 0.9489\n",
            "Train datta Epoch 8 Batch 200 Loss 1.0096\n",
            "Train datta Epoch 8 Batch 400 Loss 0.9640\n",
            "Train datta Epoch 8 Batch 600 Loss 0.8763\n",
            "validation datta Epoch 8 Batch 0 Loss 0.9327\n",
            "Epoch 8 Loss 0.0142\n",
            "Epoch 8 Loss 0.0138\n",
            "Train datta Epoch 9 Batch 0 Loss 0.8110\n",
            "Train datta Epoch 9 Batch 200 Loss 0.9062\n",
            "Train datta Epoch 9 Batch 400 Loss 0.7956\n",
            "Train datta Epoch 9 Batch 600 Loss 0.8006\n",
            "validation datta Epoch 9 Batch 0 Loss 0.8984\n",
            "Epoch 9 Loss 0.0138\n",
            "Epoch 9 Loss 0.0133\n",
            "Train datta Epoch 10 Batch 0 Loss 0.7285\n",
            "Train datta Epoch 10 Batch 200 Loss 0.7901\n",
            "Train datta Epoch 10 Batch 400 Loss 0.8459\n",
            "Train datta Epoch 10 Batch 600 Loss 0.8614\n",
            "validation datta Epoch 10 Batch 0 Loss 0.9004\n",
            "Epoch 10 Loss 0.0134\n",
            "Epoch 10 Loss 0.0129\n",
            "Train datta Epoch 11 Batch 0 Loss 0.8364\n",
            "Train datta Epoch 11 Batch 200 Loss 0.8820\n",
            "Train datta Epoch 11 Batch 400 Loss 0.8715\n",
            "Train datta Epoch 11 Batch 600 Loss 0.9261\n",
            "validation datta Epoch 11 Batch 0 Loss 0.8631\n",
            "Epoch 11 Loss 0.0130\n",
            "Epoch 11 Loss 0.0125\n",
            "Train datta Epoch 12 Batch 0 Loss 0.8272\n",
            "Train datta Epoch 12 Batch 200 Loss 0.7755\n",
            "Train datta Epoch 12 Batch 400 Loss 0.8013\n",
            "Train datta Epoch 12 Batch 600 Loss 0.7368\n",
            "validation datta Epoch 12 Batch 0 Loss 0.8327\n",
            "Epoch 12 Loss 0.0126\n",
            "Epoch 12 Loss 0.0122\n",
            "Train datta Epoch 13 Batch 0 Loss 0.7398\n",
            "Train datta Epoch 13 Batch 200 Loss 0.7557\n",
            "Train datta Epoch 13 Batch 400 Loss 0.8517\n",
            "Train datta Epoch 13 Batch 600 Loss 0.6886\n",
            "validation datta Epoch 13 Batch 0 Loss 0.8039\n",
            "Epoch 13 Loss 0.0122\n",
            "Epoch 13 Loss 0.0118\n",
            "Train datta Epoch 14 Batch 0 Loss 0.8120\n",
            "Train datta Epoch 14 Batch 200 Loss 0.6712\n",
            "Train datta Epoch 14 Batch 400 Loss 0.7193\n",
            "Train datta Epoch 14 Batch 600 Loss 0.7525\n",
            "validation datta Epoch 14 Batch 0 Loss 0.7766\n",
            "Epoch 14 Loss 0.0118\n",
            "Epoch 14 Loss 0.0115\n",
            "Train datta Epoch 15 Batch 0 Loss 0.6709\n",
            "Train datta Epoch 15 Batch 200 Loss 0.7953\n",
            "Train datta Epoch 15 Batch 400 Loss 0.7483\n",
            "Train datta Epoch 15 Batch 600 Loss 0.7308\n",
            "validation datta Epoch 15 Batch 0 Loss 0.7534\n",
            "Epoch 15 Loss 0.0114\n",
            "Epoch 15 Loss 0.0110\n",
            "Train datta Epoch 16 Batch 0 Loss 0.6585\n",
            "Train datta Epoch 16 Batch 200 Loss 0.7839\n",
            "Train datta Epoch 16 Batch 400 Loss 0.6339\n",
            "Train datta Epoch 16 Batch 600 Loss 0.6307\n",
            "validation datta Epoch 16 Batch 0 Loss 0.7215\n",
            "Epoch 16 Loss 0.0109\n",
            "Epoch 16 Loss 0.0106\n",
            "Train datta Epoch 17 Batch 0 Loss 0.5817\n",
            "Train datta Epoch 17 Batch 200 Loss 0.7029\n",
            "Train datta Epoch 17 Batch 400 Loss 0.7475\n",
            "Train datta Epoch 17 Batch 600 Loss 0.6267\n",
            "validation datta Epoch 17 Batch 0 Loss 0.6874\n",
            "Epoch 17 Loss 0.0104\n",
            "Epoch 17 Loss 0.0102\n",
            "Train datta Epoch 18 Batch 0 Loss 0.6004\n",
            "Train datta Epoch 18 Batch 200 Loss 0.5118\n",
            "Train datta Epoch 18 Batch 400 Loss 0.6960\n",
            "Train datta Epoch 18 Batch 600 Loss 0.6523\n",
            "validation datta Epoch 18 Batch 0 Loss 0.6603\n",
            "Epoch 18 Loss 0.0099\n",
            "Epoch 18 Loss 0.0098\n",
            "Train datta Epoch 19 Batch 0 Loss 0.5922\n",
            "Train datta Epoch 19 Batch 200 Loss 0.5289\n",
            "Train datta Epoch 19 Batch 400 Loss 0.6302\n",
            "Train datta Epoch 19 Batch 600 Loss 0.5545\n",
            "validation datta Epoch 19 Batch 0 Loss 0.6164\n",
            "Epoch 19 Loss 0.0095\n",
            "Epoch 19 Loss 0.0091\n",
            "Train datta Epoch 20 Batch 0 Loss 0.5566\n",
            "Train datta Epoch 20 Batch 200 Loss 0.5552\n",
            "Train datta Epoch 20 Batch 400 Loss 0.6272\n",
            "Train datta Epoch 20 Batch 600 Loss 0.5727\n",
            "validation datta Epoch 20 Batch 0 Loss 0.6012\n",
            "Epoch 20 Loss 0.0089\n",
            "Epoch 20 Loss 0.0086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNkXiemq9bDO",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-DcisWA9bDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(en_text):\n",
        "        # normalize English\n",
        "        en_text = en_text.lower()\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        # en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "        return en_text\n",
        "        \n",
        "\n",
        "def evaluate(sentence):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state, _ = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui6bFxRt9bDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def result(sentence):\n",
        "    result, sentence, _= evaluate(sentence)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2duMXKX9bDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7e3dbc30-1306-42ef-f93f-99f1186b44fa"
      },
      "source": [
        "    result, sentence = result(\"he has a dog\")\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    print('BLEU corpus: %f' % sentence_bleu([\"彼は犬を飼っている\"], result))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ he has a dog _end\n",
            "Predicted translation: 彼は 犬 か _end \n",
            "BLEU corpus: 0.388273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlCu-uvtwOft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9d08859d-ad8d-4b79-80a1-d27ce0848bb9"
      },
      "source": [
        "result, sentence = result(\"What is this!?\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))\n",
        "print('BLEU corpus: %f' % sentence_bleu([\"なにこれ !?\"], result))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ what is this ! ? _end\n",
            "Predicted translation: 何 ? _end \n",
            "BLEU corpus: 0.686589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2FZwaiky-yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4b5a814d-f18a-4d6a-bfaa-64fc621aa4cc"
      },
      "source": [
        "result, sentence = result(\"Actually I have it.\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))\n",
        "print('BLEU corpus: %f' % sentence_bleu([\"実は持っている\"], result))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ actually i have it . _end\n",
            "Predicted translation: 実は 私 には _end \n",
            "BLEU corpus: 0.336493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67d09FoPz2Al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3a059e34-be13-4a91-c793-01c597b9ea91"
      },
      "source": [
        "result, sentence = result(\"she is beautiful\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))\n",
        "print('BLEU corpus: %f' % sentence_bleu([\"彼女は美しい\"], result))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ she is beautiful _end\n",
            "Predicted translation: この 美しい _end \n",
            "BLEU corpus: 0.259654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2saSpKK0Cwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}