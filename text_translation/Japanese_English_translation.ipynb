{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(path):\n",
    "    # open a txt file as read only\n",
    "    lines = io.open(path, encoding='UTF-8', errors=\"ignore\").read().strip().split('\\n')\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    lines = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w):\n",
    "    # check japanese lang\n",
    "    p = re.compile('[\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F]+')\n",
    "    if p.search(w):\n",
    "        # Morphological analysis for japanese lang\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        w = m.parse(w)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> プール に 行き たい  。  でも 今日 は 雨  . <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨.\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ja_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> you are back ,  aren t you ,  harold ? <end>', '<start> my opponent is shark . <end>', '<start> this is one thing in exchange for another . <end>', '<start> yeah ,  i m fine . <end>', '<start> don t come to the office anymore .  don t call me either . <end>', '<start> looks beautiful . <end>', '<start> get him out of here ,  because i will fucking kill him . <end>', '<start> you killed him ! <end>', '<start> okay ,  then who ? <end>', '<start> it seems a former employee .  .  . <end>')\n",
      "('<start> あなた は 戻っ た の ね ハロルド  ? <end>', '<start> 俺 の 相手 は シャーク だ  。 <end>', '<start> 引き換え だ ある 事 と ある 物 の <end>', '<start> もう いい よ ごちそうさま ううん <end>', '<start> もう 会社 に は 来 ない で くれ 電話 も する な <end>', '<start> きれい だ  。 <end>', '<start> 連れ て 行け 殺し そう だ わかっ た か  ? <end>', '<start> 殺し た の か  ! <end>', '<start> わぁ   !  いつも すみません  。  いい の よ   。 <end>', '<start> カンパニー の 元 社員 が <end>')\n"
     ]
    }
   ],
   "source": [
    "en, ja = create_lang_list(10)\n",
    "print(en[:10])\n",
    "print(ja[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # victorize a text corpus\n",
    "    lang_tokenize = tf.keras.preprocessing.text.Tokenizer(filters=' ')\n",
    "    lang_tokenize.fit_on_texts(lang)\n",
    "    # to sequence of integer  e.g. ['こんにちは 今日　は', \"today is so cold\"] →[[1, 2], [3, 4, 5, 6]]\n",
    "    tensor = lang_tokenize.texts_to_sequences(lang)\n",
    "    print(tensor)\n",
    "    # Fixed length because length of sequence of integers are different e.g. [[1, 2], [3, 4, 5, 6]] \n",
    "    # → →[[1 2 0 0] [3 4 5 6]]\n",
    "    # return (len(sequences), maxlen) \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                                                                         padding=\"post\")\n",
    "    print(tensor)\n",
    "    return tensor, lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5, 6]]\n",
      "[[1 2 0 0]\n",
      " [3 4 5 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 0, 0],\n",
       "        [3, 4, 5, 6]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1a3793af10>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize(['こんにちは 今日　は', \"today is so cold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
