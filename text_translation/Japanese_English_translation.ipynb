{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(path):\n",
    "    # open a txt file as read only\n",
    "    lines = io.open(path, encoding='UTF-8', errors=\"ignore\").read().strip().split('\\n')\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    lines = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "    # check japanese lang\n",
    "    p = re.compile('[\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F]+')\n",
    "    if p.search(w):\n",
    "        # Morphological analysis for japanese lang\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        w = m.parse(w)\n",
    "\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> プール に 行き たい  。  でも 今日 は 雨  . <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨.\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ja_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> you are back ,  aren t you ,  harold ? <end>', '<start> my opponent is shark . <end>', '<start> this is one thing in exchange for another . <end>', '<start> yeah ,  i m fine . <end>', '<start> don t come to the office anymore .  don t call me either . <end>', '<start> looks beautiful . <end>', '<start> get him out of here ,  because i will fucking kill him . <end>', '<start> you killed him ! <end>', '<start> okay ,  then who ? <end>', '<start> it seems a former employee .  .  . <end>')\n",
      "('<start> あなた は 戻っ た の ね ハロルド  ? <end>', '<start> 俺 の 相手 は シャーク だ  。 <end>', '<start> 引き換え だ ある 事 と ある 物 の <end>', '<start> もう いい よ ごちそうさま ううん <end>', '<start> もう 会社 に は 来 ない で くれ 電話 も する な <end>', '<start> きれい だ  。 <end>', '<start> 連れ て 行け 殺し そう だ わかっ た か  ? <end>', '<start> 殺し た の か  ! <end>', '<start> わぁ   !  いつも すみません  。  いい の よ   。 <end>', '<start> カンパニー の 元 社員 が <end>')\n"
     ]
    }
   ],
   "source": [
    "en, ja = create_lang_list(10)\n",
    "print(en[:10])\n",
    "print(ja[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return (len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # victorize a text corpus\n",
    "    lang_tokenize = tf.keras.preprocessing.text.Tokenizer(filters=' ')\n",
    "    lang_tokenize.fit_on_texts(lang)\n",
    "    # to sequence of integer  e.g. ['こんにちは 今日　は', \"today is so cold\"] →[[1, 2], [3, 4, 5, 6]]\n",
    "    tensor = lang_tokenize.texts_to_sequences(lang)\n",
    "    # Fixed length because length of sequence of integers are different e.g. [[1, 2], [3, 4, 5, 6]] \n",
    "    # → →[[1 2 0 0] [3 4 5 6]]\n",
    "    # return (len(sequences), maxlen) \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                                                                         padding=\"post\")\n",
    "    return tensor, lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5, 6]]\n",
      "[[1 2 0 0]\n",
      " [3 4 5 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 0, 0],\n",
       "        [3, 4, 5, 6]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1a3eba1590>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize(['こんにちは 今日　は', \"today is so cold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    output_lang, input_lang = create_lang_list(num_examples)\n",
    "    print(\"input\", input_lang[:1])\n",
    "    print(\"output\", output_lang[:1])\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(input_lang)\n",
    "    print('Total unique words in the input: %s' % len(inp_lang_tokenizer.word_index))\n",
    "    print(inp_lang_tokenizer.word_index[\"こんにちは\"])\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(output_lang)\n",
    "    print('Total unique words in the output: %s' % len(targ_lang_tokenizer.word_index))\n",
    "    print(targ_lang_tokenizer.word_index[\"hello\"])\n",
    "\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ('<start> あなた は 戻っ た の ね ハロルド  ? <end>',)\n",
      "output ('<start> you are back ,  aren t you ,  harold ? <end>',)\n",
      "Total unique words in the input: 26141\n",
      "1199\n",
      "Total unique words in the output: 21596\n",
      "429\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_example = 50000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(num_example)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000 40000 10000\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "input_tensor_train, input_tensor_val, \\\n",
    "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# show length\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "1-----><start>\n",
      "122----->彼ら\n",
      "3----->の\n",
      "1748----->歌\n",
      "9----->が\n",
      "9319----->聴ける\n",
      "3----->の\n",
      "4----->は\n",
      "470----->ホント\n",
      "1647----->久しぶり\n",
      "21----->です\n",
      "2-----><end>\n",
      "output lang: index to word mapping\n",
      "1-----><start>\n",
      "44----->they\n",
      "377----->haven\n",
      "19----->t\n",
      "104----->had\n",
      "10----->a\n",
      "1744----->fresh\n",
      "1189----->audience\n",
      "18----->in\n",
      "243----->many\n",
      "10----->a\n",
      "1492----->moon\n",
      "3----->.\n",
      "2-----><end>\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 26141\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)\n",
    "embedding_dim = 100\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "print('Total unique words in the input: %s' % len(inp_lang.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2dimention list for using word2vec\n",
    "japanese_sentence_list = []\n",
    "word2vec_list = []\n",
    "\n",
    "for i in ja:\n",
    "    i = i.replace(\"<start>\", \"\")\n",
    "    i = i.replace(\"<end>\", \"\")\n",
    "    japanese_sentence_list.append(i)\n",
    "    c = copy.deepcopy(japanese_sentence_list)\n",
    "    word2vec_list.append(c)\n",
    "    japanese_sentence_list.clear()\n",
    "    word2vec_list\n",
    "model = word2vec.Word2Vec(word2vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
