{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import os\n",
    "import io\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(path):\n",
    "    # open a txt file as read only\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    lines = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate English to Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w):\n",
    "    # check japanese lang\n",
    "    p = re.compile('[\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F]+')\n",
    "    if p.search(w):\n",
    "        # Morphological analysis for japanese lang\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        w = m.parse(w)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> プール に 行き たい  。  でも 今日 は 雨  . <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨.\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ja_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "('<start> あなた は 戻っ た の ね ハロルド  ? <end>', '<start> 俺 の 相手 は シャーク だ  。 <end>', '<start> 引き換え だ ある 事 と ある 物 の <end>', '<start> もう いい よ ごちそうさま ううん <end>', '<start> もう 会社 に は 来 ない で くれ 電話 も する な <end>', '<start> きれい だ  。 <end>', '<start> 連れ て 行け 殺し そう だ わかっ た か  ? <end>', '<start> 殺し た の か  ! <end>', '<start> わぁ   !  いつも すみません  。  いい の よ   。 <end>', '<start> カンパニー の 元 社員 が <end>')\n"
     ]
    }
   ],
   "source": [
    "en, ja = create_lang_list(1000)\n",
    "print(len(en))\n",
    "print(ja[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:1, place:2, is:3, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # transform a list of num sample into a 2D Numpy array of shape \n",
    "    # Fixed length because length of sequence of integers are different\n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3, 1, 4],\n",
       "        [5, 6, 0, 0],\n",
       "        [7, 1, 8, 9]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1a4b455cf8>)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize(['this place is good', \"hello world\", \"today is so cold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean input, output pairs\n",
    "def load_dataset(num_example):\n",
    "    input_lang, target_lang= create_lang_list(num_example)\n",
    "    input_tensor, input_lang_tokenize = tokenize(input_lang)\n",
    "    print(input_lang_tokenize.word_index[\"hello\"])\n",
    "    target_tensor, target_lang_tokenize = tokenize(target_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "# limit datasize for test\n",
    "num_example = 1000\n",
    "# get data\n",
    "input_tensor, target_tensor, input_lang, target_lang = load_dataset(num_example)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 200 800 200\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "input_tensor_train, input_tensor_val, \\\n",
    "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# show length\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "1-----><start>\n",
      "605----->asamoto\n",
      "184----->won\n",
      "19----->t\n",
      "273----->last\n",
      "38----->like\n",
      "22----->this\n",
      "16----->!\n",
      "2-----><end>\n",
      "output lang: index to word mapping\n",
      "1-----><start>\n",
      "676----->asamoto\n",
      "4----->は\n",
      "42----->この\n",
      "677----->まま\n",
      "52----->じゃ\n",
      "198----->いけ\n",
      "16----->ない\n",
      "17----->と\n",
      "136----->思う\n",
      "11----->。\n",
      "2-----><end>\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(input_lang, input_tensor_train[0])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(target_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 1817\n",
      "<DatasetV1Adapter shapes: ((47,), (32,)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 47), (64, 32)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# create a dataset\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)\n",
    "embedding_dim = 100\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang.word_index) + 1\n",
    "print('Total unique words in the input: %s' % len(input_lang.word_index))\n",
    "vocab_tar_size = len(target_lang.word_index) + 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "print(dataset)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(64), Dimension(47)]),\n",
       " TensorShape([Dimension(64), Dimension(32)]))"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch =  next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove\n",
    "def load_glove():\n",
    "    embeddings_dictionary = {}\n",
    "    glove_file = open(\"dataset/glove.6B/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "    \n",
    "    for line in glove_file:\n",
    "        records =  line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = asarray(records[1:], dtype=\"float32\")\n",
    "        embeddings_dictionary[word] =  vector_dimensions\n",
    "    glove_file.close()\n",
    "    \n",
    "    num_words = min(num_example, vocab_inp_size)\n",
    "    embedding_matrix = zeros((num_words, embedding_dim))\n",
    "    for word, index in input_lang.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embeddings_dictionary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary, embedding_matrix = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n",
      "[ 0.34220001  0.91930997  0.75269997 -0.0044284   0.028652    0.66755003\n",
      " -0.59455001  0.084591    0.022404    0.42912    -0.25150001  0.076922\n",
      "  0.37222999  0.24417     0.44373    -0.67593002  0.47753999 -0.118\n",
      " -0.57872999  0.64397001  0.059332    1.05190003  0.77245998  0.14067\n",
      "  0.34026    -1.14230001  0.078391   -0.73179001 -0.27879    -0.036137\n",
      "  0.049506   -0.038606    0.40932    -0.089937    0.58244997  0.21867\n",
      "  0.28086001  0.78882003  0.47681001 -0.58727998 -1.07000005  0.30320999\n",
      " -0.18365     0.016456    0.68322998 -0.16870999 -0.52803999  0.10356\n",
      " -0.25731999 -0.73175001 -0.076648   -0.59186     0.65042001  1.21169996\n",
      " -0.027989   -2.06279993  0.13432001 -0.68089998  1.10780001  0.12594999\n",
      "  0.30129999  0.83706999 -0.57501    -0.91003001  0.79848999  0.64849001\n",
      "  0.61818999 -0.53176999  0.2687     -0.20434    -0.11668     0.038548\n",
      " -0.24094    -0.13314     0.09822     0.50792998 -0.21667001 -0.29523\n",
      " -0.10765    -0.09545    -0.045118    0.99844998 -0.19981     0.80215001\n",
      " -0.97672999  0.60113001  0.40270999 -0.71578997 -0.64367002 -0.39008\n",
      "  0.54009998 -0.96763998  1.07910001 -0.48045999 -0.89630997 -0.22244\n",
      " -0.44832999 -0.38088     0.95863003 -0.79759002]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"hello\"])\n",
    "print(embedding_matrix[398])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
