{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(path):\n",
    "    # open a txt file as read only\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    lines = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate English to Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w):\n",
    "    # check japanese lang\n",
    "    p = re.compile('[\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F]+')\n",
    "    if p.search(w):\n",
    "        # Morphological analysis for japanese lang\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        w = m.parse(w)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> プール に 行き たい  。  でも 今日 は 雨  . <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨.\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ja_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "('<start> あなた は 戻っ た の ね ハロルド  ? <end>', '<start> 俺 の 相手 は シャーク だ  。 <end>', '<start> 引き換え だ ある 事 と ある 物 の <end>', '<start> もう いい よ ごちそうさま ううん <end>', '<start> もう 会社 に は 来 ない で くれ 電話 も する な <end>', '<start> きれい だ  。 <end>', '<start> 連れ て 行け 殺し そう だ わかっ た か  ? <end>', '<start> 殺し た の か  ! <end>', '<start> わぁ   !  いつも すみません  。  いい の よ   。 <end>', '<start> カンパニー の 元 社員 が <end>')\n"
     ]
    }
   ],
   "source": [
    "en, ja = create_lang_list(1000)\n",
    "print(len(en))\n",
    "print(ja[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:1, place:2, is:3, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # transform a list of num sample into a 2D Numpy array of shape \n",
    "    # Fixed length because length of sequence of integers are different\n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3, 1, 4],\n",
       "        [5, 6, 0, 0],\n",
       "        [7, 1, 8, 9]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1a3fccc690>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize(['this place is good', \"hello world\", \"today is so cold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean input, output pairs\n",
    "def load_dataset(num_example):\n",
    "    input_lang, target_lang= create_lang_list(num_example)\n",
    "    input_tensor, input_lang_tokenize = tokenize(input_lang)\n",
    "    print(input_lang_tokenize.word_index[\"hello\"])\n",
    "    target_tensor, target_lang_tokenize = tokenize(target_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n"
     ]
    }
   ],
   "source": [
    "# limit datasize for test\n",
    "num_example = 30000\n",
    "# get data\n",
    "input_tensor, target_tensor, input_lang, target_lang = load_dataset(num_example)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000 24000 6000\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "input_tensor_train, input_tensor_val, \\\n",
    "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# show length\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "1-----><start>\n",
      "28----->he\n",
      "75----->did\n",
      "12----->it\n",
      "38----->with\n",
      "84----->his\n",
      "243----->own\n",
      "63----->will\n",
      "16----->!\n",
      "2-----><end>\n",
      "output lang: index to word mapping\n",
      "1-----><start>\n",
      "7283----->茂\n",
      "9----->が\n",
      "865----->勝手\n",
      "5----->に\n",
      "905----->落ち\n",
      "6----->た\n",
      "3----->の\n",
      "2-----><end>\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(input_lang, input_tensor_train[0])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(target_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 16351\n",
      "<DatasetV1Adapter shapes: ((65,), (51,)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 65), (64, 51)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# create a dataset\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)\n",
    "embedding_dim = 100\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang.word_index) + 1\n",
    "print('Total unique words in the input: %s' % len(input_lang.word_index))\n",
    "vocab_tar_size = len(target_lang.word_index) + 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "print(dataset)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(64), Dimension(65)]),\n",
       " TensorShape([Dimension(64), Dimension(51)]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch =  next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    embeddings_dictionary = {}\n",
    "    glove_file = open(\"dataset/glove.6B/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "    \n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = asarray(records[1:], dtype=\"float32\")\n",
    "        # correspond word and vector\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    \n",
    "    num_words = min(num_example, vocab_inp_size)\n",
    "    embedding_matrix = zeros((num_words, embedding_dim))\n",
    "    for word, index in input_lang.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embeddings_dictionary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary, embedding_matrix = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n",
      "[ 0.26688001  0.39631999  0.61690003 -0.77451003 -0.1039      0.26697001\n",
      "  0.27880001  0.30992001  0.0054685  -0.085256    0.73602003 -0.098432\n",
      "  0.54790002 -0.030305    0.33478999  0.14094    -0.0070003   0.32569\n",
      "  0.22902     0.46557    -0.19531     0.37491    -0.71390003 -0.51775002\n",
      "  0.77038997  1.08809996 -0.66011    -0.16234     0.91189998  0.21046001\n",
      "  0.047494    1.00189996  1.11329997  0.70094001 -0.08696     0.47571\n",
      "  0.1636     -0.44468999  0.44690001 -0.93817002  0.013101    0.085964\n",
      " -0.67456001  0.49662    -0.037827   -0.11038    -0.28612     0.074606\n",
      " -0.31527001 -0.093774   -0.57068998  0.66864997  0.45307001 -0.34154001\n",
      " -0.7166     -0.75273001  0.075212    0.57902998 -0.1191     -0.11379\n",
      " -0.10026     0.71341002 -1.15740001 -0.74026     0.40452     0.18023001\n",
      "  0.21449     0.37638     0.11239    -0.53639001 -0.025092    0.31885999\n",
      " -0.25013    -0.63283002 -0.011843    1.37699997  0.86013001  0.20476\n",
      " -0.36815    -0.68874002  0.53512001 -0.46555999  0.27388999  0.4118\n",
      " -0.85399997 -0.046288    0.11304    -0.27326     0.15636    -0.20333999\n",
      "  0.53586     0.59784001  0.60469002  0.13734999  0.42232001 -0.61278999\n",
      " -0.38486001  0.35842001 -0.48464     0.30728   ]\n"
     ]
    }
   ],
   "source": [
    "# check vector \"hello\" → both of vector are almost same\n",
    "print(embeddings_dictionary[\"hello\"])\n",
    "print(embedding_matrix[397])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, embedding_matrix):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim, \n",
    "                                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state=True,\n",
    "                                                            recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "            return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 65, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, embedding_matrix)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, nuits):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                self.W1(values) +  self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention result shape: (batch size, units) (64, 1024)\n",
      "attention weight shape:(batch size, sequence_length, 1) (64, 65, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result , attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"attention weight shape:(batch size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state=True,\n",
    "                                                            recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "    \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state =  self.gru(x)\n",
    "                \n",
    "        # output shape == (batch_size * 1, hidden_size\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output  shape == (batch_size, vocab) \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return  x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output  shape:(batch_size, vocab size) (64, 20070)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _,  _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                                                        sample_hidden, sample_output)\n",
    "print(\"Decoder output  shape:(batch_size, vocab size) {}\".format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                                        encoder=encoder,\n",
    "                                                        decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "         \n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # teacher forcing - feeding the target as a next input\n",
    "        for t in range(1, target.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            dec_input  = tf.expand_dims(target[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epochs in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss =  0\n",
    "     \n",
    "    for (batch, (inp, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, target, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(\"epoch {} batch {} loss  {: .4f}\".format(epochs + 1,\n",
    "                                                                                          batch,\n",
    "                                                                                          batch_loss.numpy()))\n",
    "        # save checkpoint every 2 epochs\n",
    "        if (epochs + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print(\"epoch {} loss {: .4f}\".format(epochs + 1,\n",
    "                                                                     total_loss / steps_per_epoch))\n",
    "        print(\"time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
