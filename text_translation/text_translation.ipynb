{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(filename):\n",
    "    # open a txt file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    lines = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a words english and japanese\n",
    "def split_each_languages(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lang_list():\n",
    "    # load txt file\n",
    "    text = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    # recieve jap and eng sentence as list\n",
    "    pairs = split_each_languages(text)\n",
    "\n",
    "    english_list = list()\n",
    "    japanese_list = list()\n",
    "    # create each languages list\n",
    "    for i in pairs:\n",
    "        english_list.append(i[0])\n",
    "        japanese_list.append(i[1])\n",
    "\n",
    "    return (english_list, japanese_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang list\n",
    "en, ja = create_lang_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate English to Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    # avoid lang accent check → japanese has lang acccent\n",
    "    return ' '.join(c for c in unicodedata.normalize('NFC', s))\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w, lang):\n",
    "    # Morphological analysis for japanese lang\n",
    "    if lang == \"ja\":\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        w = m.parse(w)\n",
    "    \n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> m a y   i   b o r r o w   t h i s   b o o k  ? <end>\n",
      "<start> プ ー ル   に   行 き   た い    。    で も   今 日   は   雨 <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨\"\n",
    "print(preprocess_sentence(en_sentence, \"en\"))\n",
    "print(preprocess_sentence(ja_sentence, \"ja\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> d o   y o u   w a n t   t o   h e a r   i t  ? <end>\n",
      "<start> 聞 き   た く   な け れ   ば   言 わ   な い   け れ ど    。 <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(en[-1], \"en\"))\n",
    "print(preprocess_sentence(ja[-1], \"ja\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus     \n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:1, place:2, is:3, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    print(lang_tokenizer)\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    # e.g. {this:1, place:2, is:3, good:4} → [[1, 2, 3, 4]]\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    print(tensor)\n",
    "    # transform a list of num sample into a 2D Numpy array of shape \n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    print(tensor)\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x1526afa20>\n",
      "[[1, 2, 3, 4]]\n",
      "[[1 2 3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3, 4]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1526afa20>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize([\"this place is good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
