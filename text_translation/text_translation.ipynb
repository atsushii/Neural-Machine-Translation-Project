{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "text_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB-6WuKgAmq5",
        "colab_type": "code",
        "outputId": "ea516b3d-883c-477e-c4a4-f47eaea9f97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "! pip install sentencepiece\n",
        "\n",
        "import sentencepiece as spm\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.83)\n",
            "Requirement already satisfied: japanize-matplotlib in /usr/local/lib/python3.6/dist-packages (1.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7Ykqf5AmrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load txt file\n",
        "def load_def(path):\n",
        "    # open a txt file as read only\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMBPNGqbAmrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines = load_def(\"/content/drive/My Drive/Colab Notebooks/raw.txt\")\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwoFejoMB56z",
        "colab_type": "code",
        "outputId": "531d0285-db0e-4251-d611-bfac90b26e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiRLa8A2AmrQ",
        "colab_type": "text"
      },
      "source": [
        "# translate English to Japanese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5btFO26fAmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "# train SentencePiece using english text \n",
        "spm.SentencePieceTrainer.Train('--input=english.txt --model_prefix=m --vocab_size=8000')\n",
        "# convert unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
        "                    if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# preprocess\n",
        "def preprocess_sentence(w):\n",
        "    # check japanese lang\n",
        "    # p = re.compile('[\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F]+')\n",
        "    # if p.search(w):\n",
        "        # Morphological analysis for japanese lang\n",
        "        # english sentencepiece\n",
        "\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    # create a space between word and the punctuation\n",
        "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
        "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
        "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # add a start and end  token to the sentence\n",
        "    # model know when to start and end\n",
        "    w = \"<start> \" + w + \" <end>\"\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6S5BeTsAmra",
        "colab_type": "code",
        "outputId": "fca6fbf1-34dc-4c97-fb6a-e3e00745d9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check word\n",
        "en_sentence =u\"May I borrow this book?\"\n",
        "ja_sentence = u\"プールに行きたい。でも今日は雨.\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(ja_sentence))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> プールに行きたい 。 でも今日は雨 . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g12RssEAmrj",
        "colab_type": "code",
        "outputId": "5c226848-2599-4789-ce25-b1bc3a15563e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "en, ja = create_lang_list(100)\n",
        "print(len(en))\n",
        "print(ja[:10])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "('<start> あなたは戻ったのね ハロルド ? <end>', '<start> 俺の相手は シャークだ 。 <end>', '<start> 引き換えだ ある事とある物の <end>', '<start> もういいよ ごちそうさま ううん <end>', '<start> もう会社には来ないでくれ 電話もするな <end>', '<start> きれいだ 。 <end>', '<start> 連れて行け 殺しそうだ わかったか ? <end>', '<start> 殺したのか ! <end>', '<start> わぁ  !  いつも すみません 。  いいのよ  。 <end>', '<start> カンパニーの元社員が <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvnpy3CwAmro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8BX1BNgAmru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:1, place:2, is:3, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTO5p6jxAmr2",
        "colab_type": "code",
        "outputId": "029a4daa-0fa5-4d4f-bf6f-00b57cff0106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# example\n",
        "tokenize(['this place is good', \"こんにちは 今日は いい天気 今日\", \"today is so cold\"])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  1,  4],\n",
              "        [ 5,  6,  7,  8],\n",
              "        [ 9,  1, 10, 11]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7f608a58b630>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62H3DewpAms7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a clean input, output pairs\n",
        "def load_dataset(num_example):\n",
        "    input_lang, target_lang= create_lang_list(num_example)\n",
        "    sp.Load('/content/drive/My Drive/wiki-ja.model')\n",
        "    target_lang = [\"\".join(sp.encode_as_pieces(i)).replace(\"▁\", \" \") for i in target_lang]\n",
        "    print(target_lang[:10])\n",
        "    input_tensor, input_lang_tokenize = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenize = tokenize(target_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9hXwjOqAmtA",
        "colab_type": "code",
        "outputId": "acdd9986-b20f-4e67-bb7e-82b125ed4573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# limit datasize for test\n",
        "num_example = 10000\n",
        "# get data\n",
        "input_tensor, target_tensor, input_lang, target_lang = load_dataset(num_example)\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' <start> あなたは戻ったのね ハロルド ? <end>', ' <start> 俺の相手は シャークだ 。 <end>', ' <start> 引き換えだ ある事とある物の <end>', ' <start> もういいよ ごちそうさま ううん <end>', ' <start> もう会社には来ないでくれ 電話もするな <end>', ' <start> きれいだ 。 <end>', ' <start> 連れて行け 殺しそうだ わかったか ? <end>', ' <start> 殺したのか ! <end>', ' <start> わぁ ! いつも すみません 。 いいのよ 。 <end>', ' <start> カンパニーの元社員が <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4ppmsvAmtC",
        "colab_type": "code",
        "outputId": "cae8222b-e2f2-4553-d8c0-28a402cd8d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create trainnig set and validation set\n",
        "input_tensor_train, input_tensor_val, \\\n",
        "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# show length\n",
        "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 2000 8000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbT9xCEZAmtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZasaMQBAmtO",
        "colab_type": "code",
        "outputId": "56887f64-89ce-470c-cecb-0f3820cf63ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[3])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang, target_tensor_train[3])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1-----><start>\n",
            "21----->this\n",
            "6550----->script\n",
            "260----->wasn\n",
            "18----->t\n",
            "1015----->written\n",
            "110----->by\n",
            "6551----->takakura\n",
            "6552----->yuuji\n",
            "3----->.\n",
            "2-----><end>\n",
            "output lang: index to word mapping\n",
            "1-----><start>\n",
            "9703----->この脚本は\n",
            "9704----->高倉\n",
            "9705----->雄二が\n",
            "9706----->書いたものでは\n",
            "719----->ありません\n",
            "5----->。\n",
            "2-----><end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Hat3fQAmtV",
        "colab_type": "code",
        "outputId": "8cf0470b-fed2-4505-99ff-b46763e87e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# create a dataset\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)\n",
        "embedding_dim = 100\n",
        "units = 512\n",
        "vocab_inp_size = len(input_lang.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang.word_index))\n",
        "vocab_tar_size = len(target_lang.word_index) + 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "print(dataset)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unique words in the input: 8560\n",
            "<DatasetV1Adapter shapes: ((55,), (16,)), types: (tf.int32, tf.int32)>\n",
            "<DatasetV1Adapter shapes: ((128, 55), (128, 16)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1i8rLmjAmtY",
        "colab_type": "code",
        "outputId": "3a24afe5-27fe-44a9-b9d9-071ef858937f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_input_batch, example_target_batch =  next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(128), Dimension(55)]),\n",
              " TensorShape([Dimension(128), Dimension(16)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGftZqkCAmta",
        "colab_type": "text"
      },
      "source": [
        "# encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVJSGkhaAmtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove():\n",
        "    embeddings_dictionary = {}\n",
        "    glove_file = open(\"/content/drive/My Drive/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
        "    \n",
        "    for line in glove_file:\n",
        "        records = line.split()\n",
        "        word = records[0]\n",
        "        vector_dimensions = asarray(records[1:], dtype=\"float32\")\n",
        "        # correspond word and vector\n",
        "        embeddings_dictionary[word] = vector_dimensions\n",
        "    glove_file.close()\n",
        "    \n",
        "    num_words = min(num_example, vocab_inp_size)\n",
        "    embedding_matrix = zeros((num_words, embedding_dim))\n",
        "    for word, index in input_lang.word_index.items():\n",
        "        embedding_vector = embeddings_dictionary.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "    return embeddings_dictionary, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2F1r2AxAmte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_dictionary, embedding_matrix = load_glove()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sotctcv-Amtg",
        "colab_type": "code",
        "outputId": "9dedf628-78ff-4de8-d7a1-b1f42b7b4571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# check vector \"hello\" → both of vector are almost same\n",
        "print(embeddings_dictionary[\"today\"])\n",
        "print(embedding_matrix[397])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.19939    0.37846    0.52093    0.28347   -0.1898    -0.20947\n",
            " -0.23286    0.14185   -0.034916  -0.36006   -0.0046717 -0.25207\n",
            "  0.31364   -0.34879    0.032268  -0.45078    0.011292   0.090903\n",
            " -0.62866    0.0079579  0.28065    0.34158   -0.25593    0.11521\n",
            "  0.10571   -0.45827    0.32193   -0.29186    0.11443    0.17972\n",
            " -0.31633    0.40085   -0.24405   -0.050205   0.16485    0.5001\n",
            "  0.11756    0.043875  -0.060235  -0.66571   -0.40628   -0.21691\n",
            "  0.068156  -0.38058   -0.4512    -0.26966    0.45961   -0.23446\n",
            " -0.26416   -1.1617     0.18417   -0.53088    0.54179    0.78295\n",
            " -0.54864   -2.0634    -0.63427    0.095073   1.9649     0.47031\n",
            " -0.54401    0.73015   -0.34352   -0.43033    0.27555    0.025185\n",
            "  0.35053    0.46295    0.40837   -0.011836   0.34553   -0.17297\n",
            " -0.13765   -0.23182    0.38986    0.031124  -0.12248    0.25285\n",
            " -1.0669    -0.19626    0.42168   -0.077544  -0.37391   -0.032836\n",
            " -0.94477   -0.10812   -0.41055   -0.52736   -0.038137  -0.03078\n",
            "  0.65461   -0.28335   -0.15259    0.42598   -1.1377     0.12824\n",
            " -0.17771   -0.53718    0.63885    0.57955  ]\n",
            "[ 0.20079     0.40580001  0.77631003 -0.31468999 -0.14511    -0.36910999\n",
            " -0.72640997  0.31404999  0.40871     0.094652   -0.00878     0.16737001\n",
            "  0.13116001  0.070899   -0.51283002  0.11571     0.26385999  0.21411\n",
            " -0.37412     0.82608998 -0.10606    -0.0072441  -0.29945999 -0.30641001\n",
            " -0.19857     0.123      -0.50041002 -0.99844998  0.50005001 -0.0052118\n",
            " -0.33855999  1.30519998  0.046308    0.21681     0.46353999  0.38295999\n",
            "  0.48480999 -0.39921001 -0.10791    -0.059285   -0.65214002 -0.055877\n",
            "  0.74448001 -0.94998002 -0.74315     0.03036    -0.29347    -0.19839001\n",
            "  0.13838001 -0.68167001  0.12497    -0.23299     0.066412    0.76749998\n",
            "  0.034816   -2.2184     -0.047262   -0.065278    1.32690001  0.28354999\n",
            "  0.60389    -0.16268    -0.31567001 -0.4991      0.90065998  0.28589001\n",
            "  0.24504     1.02550006 -0.57630002  0.089612    0.93223    -0.68361002\n",
            " -0.53608    -0.52339     0.39008    -0.14249    -0.18978    -0.27156001\n",
            " -0.72145998  0.30708     0.50988001 -0.29776999 -0.29565999 -0.025001\n",
            " -0.83491999 -0.14621     0.018213    0.43046999  0.16457    -0.19317999\n",
            "  0.22126999 -0.1121      0.066509   -0.89288002 -0.61884999  0.14035\n",
            "  0.083618    0.08112     0.18092     0.52318001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBO3QwTxAmti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, embedding_matrix):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim, \n",
        "                                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L23xj2hyAmtq",
        "colab_type": "code",
        "outputId": "74c86b19-196c-4145-9941-c933526a072a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, embedding_matrix)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 55, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNGUXvFbAmtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, nuits):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                self.W1(values) +  self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv2JZN37Amtw",
        "colab_type": "code",
        "outputId": "f83efa31-3859-40dc-ae8f-4f1a97d867fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result , attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"attention weight shape:(batch size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention result shape: (batch size, units) (128, 512)\n",
            "attention weight shape:(batch size, sequence_length, 1) (128, 55, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ef0OtQTAmty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state =  self.gru(x)\n",
        "                \n",
        "        # output shape == (batch_size * 1, hidden_size\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output  shape == (batch_size, vocab) \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return  x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ9hSTKLAmt0",
        "colab_type": "code",
        "outputId": "743edf1a-5399-4fbd-f9d2-f31f349fbe4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _,  _ = decoder(tf.random.uniform((128, 1)),\n",
        "                                                                        sample_hidden, sample_output)\n",
        "print(\"Decoder output  shape:(batch_size, vocab size) {}\".format(sample_decoder_output.shape))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output  shape:(batch_size, vocab size) (128, 15407)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_9wQJ4Amt2",
        "colab_type": "text"
      },
      "source": [
        "# Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_vXWW8QAmt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpiS09v9Amt4",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ7wO0QsAmt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint_dir = '/training_checkpoints'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                                                         encoder=encoder,\n",
        "#                                                         decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStYXjM8Amt8",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjKE0rhgAmt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target, enc_hidden):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "         \n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "        \n",
        "        # teacher forcing - feeding the target as a next input\n",
        "        for t in range(1, target.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            \n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "            \n",
        "            # using teacher forcing\n",
        "            dec_input  = tf.expand_dims(target[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(target.shape[1]))\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9zamvLvOAmt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "for epochs in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss =  0\n",
        "     \n",
        "    for (batch, (inp, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, target, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "#         if batch % 100 == 0:\n",
        "#             print(\"epoch {} batch {} loss  {: .4f}\".format(epochs + 1,\n",
        "#                                                                                           batch,\n",
        "#                                                                                           batch_loss.numpy()))\n",
        "#         # save checkpoint every 2 epochs\n",
        "#         if (epochs + 1) % 2 == 0:\n",
        "#             checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "            \n",
        "        # print(\"epoch {} loss {: .4f}\".format(epochs + 1,\n",
        "        #                                                              total_loss / steps_per_epoch))\n",
        "        # print(\"time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqylAfpDAmuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [input_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_target):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwDBVbl4NJii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge2Hv6NcNLXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKv0QifNM3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(u'what do you want?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwqccCKZNPvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}