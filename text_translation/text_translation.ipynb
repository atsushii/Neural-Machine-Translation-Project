{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "text_translatio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpvRaWWKWQaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "ef340a13-cdb0-4e07-ca9f-280dc5c26353"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as ps\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import nltk\n",
        "import io\n",
        "import re\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import unicodedata\n",
        "\n",
        "# plot japanese lang\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/c0/b75d434be51a8cc11d2e9b36f2d7f93a1bcf63bde24dc79a61d329d60b2a/japanize-matplotlib-1.0.5.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 2.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.0.5-cp36-none-any.whl size=4118721 sha256=3e31c0d98d0bb964c0766f4d5864e5b35bca9ea1b0baed604a16c7d249d03e1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/8a/08/4a784957da9f3c2b4839b4986be2fba2a481877318948be52c\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0rNelZGWQaF",
        "colab_type": "text"
      },
      "source": [
        "# load text file\n",
        "\n",
        "this dataset is aleady implemented a SentenceSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anns6lu5WeDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7580dcab-c9cf-43a4-ade9-b21053c48d02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRinpTNyWQaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"/content/drive/My Drive/Colab Notebooks/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPbz0C-VWQaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "61900656-f48e-40e2-d2e8-15e88e4944c9"
      },
      "source": [
        "# e.g.\n",
        "english_text, japanese_text = create_lang_list(20)\n",
        "english_text[:11]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"you are back, aren't you, harold?\",\n",
              " 'my opponent is shark.',\n",
              " 'this is one thing in exchange for another.',\n",
              " \"yeah, i'm fine.\",\n",
              " \"don't come to the office anymore. don't call me either.\",\n",
              " 'looks beautiful.',\n",
              " 'get him out of here, because i will fucking kill him.',\n",
              " 'you killed him!',\n",
              " 'okay, then who?',\n",
              " 'it seems a former employee...',\n",
              " 'so where are they?')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYc_BptzWQaM",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        " **Removing accented characters**\n",
        " e.g. é → e.\n",
        " \n",
        " **Expanding Contractions**\n",
        "e.g. don't → do not, I'd → I would\n",
        "\n",
        "\n",
        "**remove special word**\n",
        "e.g. remove \"123#@!\"\n",
        "\n",
        "\n",
        "**Stemming**\n",
        "e.g. corder, codes → code\n",
        "\n",
        "\n",
        "**Lemmatization**\n",
        "e.g. better → good\n",
        "\n",
        "\n",
        "**Tokenize**\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yH524sQWQaN",
        "colab_type": "text"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent\n",
        "I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycoj9N_EWQaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH0M244NWQaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d1e1946-1fe4-4d94-e6d1-206ac0cd4ef8"
      },
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('こんにちは。今日は', 'Hello world e ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJxQc8jWQaT",
        "colab_type": "text"
      },
      "source": [
        "# Expanding Contractions\n",
        "\n",
        "Japanese doesn't have a Contraction words so I just create a one function to expand Contractions for Engish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTKFj01wWQaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_constraction(text):\n",
        "\n",
        "    #  dic for expand constraction words\n",
        "    constraction_dict= {\n",
        "        \"ain't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he he will have\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'd've\": \"I would have\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'd've\": \"i would have\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'll've\": \"i will have\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"needn't've\": \"need not have\",\n",
        "        \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"shan't've\": \"shall not have\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\",\n",
        "        \"so've\": \"so have\",\n",
        "        \"so's\": \"so as\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\",\n",
        "        \"there'd've\": \"there would have\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\",\n",
        "        \"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\n",
        "        \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'd've\": \"you would have\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    #  define match pattern\n",
        "    #  IGNORECASE → no matter if word is lowercase or uppercase\n",
        "    #  DOTAIL → . is going to match \\n\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(constraction_dict.keys())),\n",
        "                                                  flags=re.IGNORECASE | re.DOTALL)\n",
        "    #  expand words\n",
        "    def expand_match(constraction):\n",
        "        # get constraction word\n",
        "        match = constraction.group(0)\n",
        "        first_char = match[0]\n",
        "        #  get expand word from constraction dict\n",
        "        expand_constraction = constraction_dict.get(match)\\\n",
        "                                                    if constraction_dict.get(match) \\\n",
        "                                                    else constraction_dict.get(match.lower())\n",
        "        \n",
        "        # create expand constraction\n",
        "        expand_constraction = first_char + expand_constraction[1:]\n",
        "        return expand_constraction\n",
        "    \n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSycujFCWQaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf0168e4-68fb-47a6-9d61-28fcf5cf6dd0"
      },
      "source": [
        "# e.g.\n",
        "expand_constraction(\"you're good I'd like to go he's she's\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you are good I would like to go he is she is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_G23bF9WQaY",
        "colab_type": "text"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
        "create space between word and punctuation (? ! . , 、 。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtD0j7FnWQaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_special_character_to_space(text):\n",
        "    pattern = r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\-/\\s]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = re.sub(r\"([?!.,。、])\", r\" \\1 \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwYZhA3HWQab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "435a92be-c601-45bd-b0b2-39dc872c4a3b"
      },
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello, . #@…123world.\"), replace_special_character_to_space(\"こん・にちは。・ いい天気。\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hello  world', 'こんにちは いい天気')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcGsO_WcWQae",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "\n",
        "I will do stemming only english which can create a base form of a word from a given word.\n",
        "Japanese language doesn't need a stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHGyieyWQaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemmer_word(text):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XszC5pOrWQaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4159a774-9a30-4183-cef7-5f3a702eea4f"
      },
      "source": [
        "# e.g.\n",
        "stemmer_word(\"hello world she has cat but he had dogs he is went to traveling\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world she ha cat but he had dog he is went to travel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xx1S2YzWQan",
        "colab_type": "text"
      },
      "source": [
        "# Text normalize\n",
        "\n",
        "I will normalize English text and Japanese text using function which is defined so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3wag5__WQan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = ()\n",
        "    target_value = ()\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space(en_text)\n",
        "\n",
        "        en_text = \"<sos> \" + en_text + \" <eos>\"\n",
        "        \n",
        "        input_value += (en_text, )\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = expand_constraction(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "\n",
        "        ja_text = \"<sos> \" + ja_text + \" <eos>\"\n",
        "        \n",
        "        target_value += (ja_text, )\n",
        "\n",
        "    return input_value, target_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRMXHMZ4WQaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_value, target_value = normalize_english(english_text, japanese_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiA2OP_jWQas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "44bd198e-3b3e-4c4e-b8b9-cde9df3cee58"
      },
      "source": [
        "print(input_value[:3])\n",
        "print(target_value[:3])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<sos> you are back are not you harold ?  <eos>', '<sos> my opponent is shark <eos>', '<sos> this is one thing in exchange for another <eos>')\n",
            "('<sos> あなたは戻ったのね ハロルド ?  <eos>', '<sos> 俺の相手は シャークだ <eos>', '<sos> 引き換えだ ある事とある物の <eos>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zeL4bGjWQav",
        "colab_type": "text"
      },
      "source": [
        "# tokenize\n",
        "tokenize each language word based on space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcmGdYw9WQaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFTQv-lGWQa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "27e47c11-264c-447b-bd58-6a726415d80a"
      },
      "source": [
        "# e.g.\n",
        "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  1,  4],\n",
              "        [ 5,  6,  7,  8],\n",
              "        [ 9,  1, 10, 11]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7f611e075b70>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKdlD3X8WQa5",
        "colab_type": "text"
      },
      "source": [
        "# create clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG0zMlpFWQa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleate a clean dataset\n",
        "def create_dataset(num_example):\n",
        "    # load a text data\n",
        "    english_text, japanese_text = create_lang_list(num_example)\n",
        "    # normalize text data     \n",
        "    input_lang, target_lang = normalize_english(english_text, japanese_text)\n",
        "    \n",
        "    # input_tensor, target_tensor: 2d numpy array\n",
        "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
        "    input_tensor, input_lang_tokenize = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenize = tokenize(target_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9TcZENDWQa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return max length\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VecIGjBNWQbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# limit datasize for test\n",
        "num_example = 20000\n",
        "# get data\n",
        "input_tensor, target_tensor, input_lang, target_lang = create_dataset(num_example)\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0AF7uONWQbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1051dc2-1c64-4240-da4a-b3dfa0694410"
      },
      "source": [
        "# create trainnig set and validation set\n",
        "input_tensor_train, input_tensor_val, \\\n",
        "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# show length\n",
        "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000 4000 16000 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz_eXcEFWQbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0R7s3inWQbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ab695382-0ad4-4547-a923-de85e6d9f792"
      },
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[10])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang, target_tensor_train[10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1-----><sos>\n",
            "36----->no\n",
            "7----->?\n",
            "20----->what\n",
            "45----->about\n",
            "704----->leaving\n",
            "3659----->witnesses\n",
            "47----->at\n",
            "9----->a\n",
            "610----->murder\n",
            "842----->scene\n",
            "7----->?\n",
            "2-----><eos>\n",
            "output lang: index to word mapping\n",
            "1-----><sos>\n",
            "23211----->殺しの目撃者を\n",
            "23212----->放っておいたそうだな\n",
            "2-----><eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwWB5bEkWQbK",
        "colab_type": "text"
      },
      "source": [
        "# define parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLcuTSOcWQbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40914918-251f-45a6-f206-0c321c1f0a22"
      },
      "source": [
        "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# if None steps_per_epoch == mum of dataset\n",
        "steps_per_epoch = len(input_tensor_train)\n",
        "embedding_dim = 300\n",
        "units = 512\n",
        "vocab_inp_size = len(input_lang.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang.word_index))\n",
        "vocab_tar_size = len(target_lang.word_index) + 1\n",
        "\n",
        "# create dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "print(dataset)\n",
        "# split dataset → (dataset % BATCH_SIZE)\n",
        "# drop_remainder=True → rounddown\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unique words in the input: 13249\n",
            "<DatasetV1Adapter shapes: ((45,), (18,)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwKE-SoWQbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8faa7d96-9a98-427c-e5c6-d1a42e35fedf"
      },
      "source": [
        "example_input_batch, example_target_batch =  next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(128), Dimension(45)]),\n",
              " TensorShape([Dimension(128), Dimension(18)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHN83wAJWQbQ",
        "colab_type": "text"
      },
      "source": [
        "# load Word2Vec pre train model\n",
        "\n",
        "I will use pre-train model for embedding for English and Japanese words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC86XnJZWQbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "98b0549e-3964-40f5-c7dc-ebcf7547bc42"
      },
      "source": [
        "import gensim\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "\n",
        "# load pre-trian model for Japanese \n",
        "ja_word2vec_model = gensim.models.Word2Vec.load('/content/drive/My Drive/jawiki.doc2vec.dbow300d/jawiki.doc2vec.dbow300d.model')\n",
        "\n",
        "# load pre-train model for English\n",
        "en_word2vec_model = word2vec.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/GoogleNews-vectors-negative300.bin\", binary=True, unicode_errors='ignore')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_cj01wWQbU",
        "colab_type": "text"
      },
      "source": [
        "**explore japanese Word2Vec pre-train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqshuRYWQbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "23f8f1f8-f096-44e4-bf57-1e4873936c4a"
      },
      "source": [
        "# similar word with country name\n",
        "ja_word2vec_model.wv.most_similar(\"日本\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('アメリカ', 0.6172695159912109),\n",
              " ('日本国内', 0.6169887781143188),\n",
              " ('米国', 0.5836105346679688),\n",
              " ('韓国', 0.583561897277832),\n",
              " ('海外', 0.5563584566116333),\n",
              " ('アジア', 0.5504777431488037),\n",
              " ('中国', 0.5415611863136292),\n",
              " ('アジア地域', 0.5271817445755005),\n",
              " ('台湾', 0.52516770362854),\n",
              " ('アジア諸国', 0.5203936100006104)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmIXKzF4WQbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "52e3ef95-c2e4-45db-e999-ea6bf8165b21"
      },
      "source": [
        "# calculate word and word\n",
        "ja_word2vec_model.wv.most_similar(positive=[\"お父さん\", \"死\"], negative=[\"泣\"])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('息子', 0.5948961973190308),\n",
              " ('お母さん', 0.5658013820648193),\n",
              " ('母親', 0.559698760509491),\n",
              " ('おばあちゃん', 0.5565805435180664),\n",
              " ('父', 0.5533062815666199),\n",
              " ('母', 0.5530043840408325),\n",
              " ('パパ', 0.5508877635002136),\n",
              " ('亡き', 0.5500179529190063),\n",
              " ('婚約者', 0.5495285391807556),\n",
              " ('死ん', 0.546082615852356)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3t9mYSiWQbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "ff9d78b4-7bce-4793-9e57-1af59c24815a"
      },
      "source": [
        "# similar word with country name\n",
        "en_word2vec_model.most_similar(\"Canada\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Canadian', 0.7513011693954468),\n",
              " ('Ontario', 0.6928848028182983),\n",
              " ('Nova_Scotia', 0.6792765259742737),\n",
              " ('Manitoba', 0.67861008644104),\n",
              " ('Alberta', 0.6736730337142944),\n",
              " ('Canadians', 0.6654781103134155),\n",
              " ('Quebec', 0.6514720320701599),\n",
              " ('British_Columbia', 0.6478375196456909),\n",
              " ('Peller_Estates_Icewine', 0.6433447003364563),\n",
              " ('Saskatchewan', 0.6383945941925049)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxCj9noHWQbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "9d3b354e-0922-4313-8b12-9d15c60663e7"
      },
      "source": [
        "# calculate word and word\n",
        "en_word2vec_model.most_similar(positive=[\"programmer\", \"skill\"], negative=[\"money\"])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Programmer', 0.4614413380622864),\n",
              " ('3D_modeller', 0.4489377737045288),\n",
              " ('skils', 0.4471549987792969),\n",
              " ('puzzle_solver', 0.4467487633228302),\n",
              " ('Jon_Shiring', 0.4382207989692688),\n",
              " ('programmers', 0.4363787770271301),\n",
              " ('coder', 0.434060662984848),\n",
              " ('skills', 0.42342013120651245),\n",
              " ('skillset', 0.42207223176956177),\n",
              " ('animator', 0.4214385449886322)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDPBubAWQbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get weights\n",
        "# English\n",
        "input_embedding_matrix = np.zeros((vocab_inp_size, embedding_dim))\n",
        "for word, i in input_lang.word_index.items():\n",
        "    if i >= num_example:\n",
        "        continue\n",
        "    try:\n",
        "        input_embedding_matrix[i] = en_word2vec_model.get_vector(word)\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "# Japanese\n",
        "target_embedding_matrix = np.zeros((vocab_tar_size, embedding_dim))\n",
        "for word, i in target_lang.word_index.items():\n",
        "    if i >= num_example:\n",
        "        continue\n",
        "    try:\n",
        "        target_embedding_matrix[i] = ja_word2vec_model.wv.get_vector(word)\n",
        "    except KeyError:\n",
        "        continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7d8AfXVWQbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a671bcab-6a51-4567-897f-185661baa759"
      },
      "source": [
        "input_embedding_matrix.shape, target_embedding_matrix.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13250, 300), (28963, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcpP69zAWQbl",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErEFSwoZWQbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, input_embedding_matrix):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim, \n",
        "                                                   weights=[input_embedding_matrix], trainable=True)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xENBAHrWQbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "36d292a3-5c50-4bde-c208-9309b16a66f1"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, input_embedding_matrix)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 45, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF9DEXuZWQbp",
        "colab_type": "text"
      },
      "source": [
        "# attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnXhUenVWQbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, nuits):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                self.W1(values) +  self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_CTm13sWQbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97252e51-9298-4890-f873-bffa63adec1c"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result , attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"attention weight shape:(batch size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention result shape: (batch size, units) (128, 512)\n",
            "attention weight shape:(batch size, sequence_length, 1) (128, 45, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52Z1zgUWQbu",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHEw2SVoWQbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, target_embedding_matrix):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim, \n",
        "                                                   weights=[target_embedding_matrix], trainable=True)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state =  self.gru(x)\n",
        "                \n",
        "        # output shape == (batch_size * 1, hidden_size\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output  shape == (batch_size, vocab) \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return  x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_LLrkT7WQb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "634bff31-e054-4a0d-da6e-8b61d3feb835"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, target_embedding_matrix)\n",
        "sample_decoder_output, _,  _ = decoder(tf.random.uniform((128, 1)),\n",
        "                                                                        sample_hidden, sample_output)\n",
        "print(\"Decoder output  shape:(batch_size, vocab size) {}\".format(sample_decoder_output.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output  shape:(batch_size, vocab size) (128, 28963)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyxl9sHzWQb3",
        "colab_type": "text"
      },
      "source": [
        "# Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckpJoUFUWQb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H4NFOFiWQb6",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqzB5XV8WQb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUGbAl1zWQcB",
        "colab_type": "text"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8QLCcGHWQcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target, enc_hidden):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "         \n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([target_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "        \n",
        "        # teacher forcing - feeding the target as a next input\n",
        "        for t in range(1, target.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input  = tf.expand_dims(target[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(target.shape[1]))\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR26DNbTWQcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cf8a241-415b-408e-bac9-de38d40ba630"
      },
      "source": [
        "import time\n",
        "EPOCHS = 15\n",
        "# load model\n",
        "# status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
        "for epochs in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss =  0\n",
        "     \n",
        "    for (batch, (inp, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, target, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            print(\"epoch {} batch {} loss  {: .4f}\".format(epochs + 1, batch, batch_loss.numpy()))\n",
        "        if (epochs + 1) % 10 == 0:\n",
        "            print(\"epoch {} loss {: .4f}\".format(epochs + 1, total_loss / steps_per_epoch))\n",
        "            print(\"time taken for 1 epoch {} sec\\n\".format(time.time() - start))\n",
        "# checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "epoch 1 batch 0 loss   1.6720\n",
            "epoch 1 batch 10 loss   1.2357\n",
            "epoch 1 batch 20 loss   1.3297\n",
            "epoch 1 batch 30 loss   1.4269\n",
            "epoch 1 batch 40 loss   1.2452\n",
            "epoch 1 batch 50 loss   1.3997\n",
            "epoch 1 batch 60 loss   1.3942\n",
            "epoch 1 batch 70 loss   1.4065\n",
            "epoch 1 batch 80 loss   1.3546\n",
            "epoch 1 batch 90 loss   1.2868\n",
            "epoch 1 batch 100 loss   1.4895\n",
            "epoch 1 batch 110 loss   1.4386\n",
            "epoch 1 batch 120 loss   1.4002\n",
            "epoch 2 batch 0 loss   1.3636\n",
            "epoch 2 batch 10 loss   1.1568\n",
            "epoch 2 batch 20 loss   1.1706\n",
            "epoch 2 batch 30 loss   1.2628\n",
            "epoch 2 batch 40 loss   1.1119\n",
            "epoch 2 batch 50 loss   1.2606\n",
            "epoch 2 batch 60 loss   1.2456\n",
            "epoch 2 batch 70 loss   1.2597\n",
            "epoch 2 batch 80 loss   1.2014\n",
            "epoch 2 batch 90 loss   1.1365\n",
            "epoch 2 batch 100 loss   1.3141\n",
            "epoch 2 batch 110 loss   1.2628\n",
            "epoch 2 batch 120 loss   1.2229\n",
            "epoch 3 batch 0 loss   1.2236\n",
            "epoch 3 batch 10 loss   1.2556\n",
            "epoch 3 batch 20 loss   1.2279\n",
            "epoch 3 batch 30 loss   1.2906\n",
            "epoch 3 batch 40 loss   1.1255\n",
            "epoch 3 batch 50 loss   1.2694\n",
            "epoch 3 batch 60 loss   1.2490\n",
            "epoch 3 batch 70 loss   1.2618\n",
            "epoch 3 batch 80 loss   1.2006\n",
            "epoch 3 batch 90 loss   1.1340\n",
            "epoch 3 batch 100 loss   1.3112\n",
            "epoch 3 batch 110 loss   1.2625\n",
            "epoch 3 batch 120 loss   1.2226\n",
            "epoch 4 batch 0 loss   1.2236\n",
            "epoch 4 batch 10 loss   1.2582\n",
            "epoch 4 batch 20 loss   1.2322\n",
            "epoch 4 batch 30 loss   1.2964\n",
            "epoch 4 batch 40 loss   1.1293\n",
            "epoch 4 batch 50 loss   1.2720\n",
            "epoch 4 batch 60 loss   1.2515\n",
            "epoch 4 batch 70 loss   1.2626\n",
            "epoch 4 batch 80 loss   1.2018\n",
            "epoch 4 batch 90 loss   1.1352\n",
            "epoch 4 batch 100 loss   1.3101\n",
            "epoch 4 batch 110 loss   1.2584\n",
            "epoch 4 batch 120 loss   1.2195\n",
            "epoch 5 batch 0 loss   1.2197\n",
            "epoch 5 batch 10 loss   1.2593\n",
            "epoch 5 batch 20 loss   1.2334\n",
            "epoch 5 batch 30 loss   1.2979\n",
            "epoch 5 batch 40 loss   1.1315\n",
            "epoch 5 batch 50 loss   1.2726\n",
            "epoch 5 batch 60 loss   1.2512\n",
            "epoch 5 batch 70 loss   1.2633\n",
            "epoch 5 batch 80 loss   1.2029\n",
            "epoch 5 batch 90 loss   1.1355\n",
            "epoch 5 batch 100 loss   1.3123\n",
            "epoch 5 batch 110 loss   1.2614\n",
            "epoch 5 batch 120 loss   1.2172\n",
            "epoch 6 batch 0 loss   1.2151\n",
            "epoch 6 batch 10 loss   1.2567\n",
            "epoch 6 batch 20 loss   1.2335\n",
            "epoch 6 batch 30 loss   1.2982\n",
            "epoch 6 batch 40 loss   1.1301\n",
            "epoch 6 batch 50 loss   1.2723\n",
            "epoch 6 batch 60 loss   1.2453\n",
            "epoch 6 batch 70 loss   1.2553\n",
            "epoch 6 batch 80 loss   1.1911\n",
            "epoch 6 batch 90 loss   1.1181\n",
            "epoch 6 batch 100 loss   1.2800\n",
            "epoch 6 batch 110 loss   1.2157\n",
            "epoch 6 batch 120 loss   1.1472\n",
            "epoch 7 batch 0 loss   1.1199\n",
            "epoch 7 batch 10 loss   1.1666\n",
            "epoch 7 batch 20 loss   1.1000\n",
            "epoch 7 batch 30 loss   1.1692\n",
            "epoch 7 batch 40 loss   1.0034\n",
            "epoch 7 batch 50 loss   1.1695\n",
            "epoch 7 batch 60 loss   1.0915\n",
            "epoch 7 batch 70 loss   1.1102\n",
            "epoch 7 batch 80 loss   1.0289\n",
            "epoch 7 batch 90 loss   0.9463\n",
            "epoch 7 batch 100 loss   1.1080\n",
            "epoch 7 batch 110 loss   1.0556\n",
            "epoch 7 batch 120 loss   1.0065\n",
            "epoch 8 batch 0 loss   1.0000\n",
            "epoch 8 batch 10 loss   1.0308\n",
            "epoch 8 batch 20 loss   1.0037\n",
            "epoch 8 batch 30 loss   1.0957\n",
            "epoch 8 batch 40 loss   0.9542\n",
            "epoch 8 batch 50 loss   1.0679\n",
            "epoch 8 batch 60 loss   1.0363\n",
            "epoch 8 batch 70 loss   1.0384\n",
            "epoch 8 batch 80 loss   0.9794\n",
            "epoch 8 batch 90 loss   0.9152\n",
            "epoch 8 batch 100 loss   1.0479\n",
            "epoch 8 batch 110 loss   0.9861\n",
            "epoch 8 batch 120 loss   0.9010\n",
            "epoch 9 batch 0 loss   0.9254\n",
            "epoch 9 batch 10 loss   0.9559\n",
            "epoch 9 batch 20 loss   0.8901\n",
            "epoch 9 batch 30 loss   0.9758\n",
            "epoch 9 batch 40 loss   0.8593\n",
            "epoch 9 batch 50 loss   0.9844\n",
            "epoch 9 batch 60 loss   0.9387\n",
            "epoch 9 batch 70 loss   0.9142\n",
            "epoch 9 batch 80 loss   0.8784\n",
            "epoch 9 batch 90 loss   0.8293\n",
            "epoch 9 batch 100 loss   0.9237\n",
            "epoch 9 batch 110 loss   0.9936\n",
            "epoch 9 batch 120 loss   0.8948\n",
            "epoch 10 batch 0 loss   0.8453\n",
            "epoch 10 loss  0.0001\n",
            "time taken for 1 epoch 0.3230149745941162 sec\n",
            "\n",
            "epoch 10 loss  0.0001\n",
            "time taken for 1 epoch 0.6452875137329102 sec\n",
            "\n",
            "epoch 10 loss  0.0002\n",
            "time taken for 1 epoch 0.9196512699127197 sec\n",
            "\n",
            "epoch 10 loss  0.0002\n",
            "time taken for 1 epoch 1.21736478805542 sec\n",
            "\n",
            "epoch 10 loss  0.0003\n",
            "time taken for 1 epoch 1.5334522724151611 sec\n",
            "\n",
            "epoch 10 loss  0.0003\n",
            "time taken for 1 epoch 1.8332922458648682 sec\n",
            "\n",
            "epoch 10 loss  0.0004\n",
            "time taken for 1 epoch 2.1451616287231445 sec\n",
            "\n",
            "epoch 10 loss  0.0004\n",
            "time taken for 1 epoch 2.4319849014282227 sec\n",
            "\n",
            "epoch 10 loss  0.0005\n",
            "time taken for 1 epoch 2.7129809856414795 sec\n",
            "\n",
            "epoch 10 loss  0.0005\n",
            "time taken for 1 epoch 3.0002212524414062 sec\n",
            "\n",
            "epoch 10 batch 10 loss   0.8661\n",
            "epoch 10 loss  0.0006\n",
            "time taken for 1 epoch 3.290829658508301 sec\n",
            "\n",
            "epoch 10 loss  0.0006\n",
            "time taken for 1 epoch 3.5817553997039795 sec\n",
            "\n",
            "epoch 10 loss  0.0007\n",
            "time taken for 1 epoch 3.869290351867676 sec\n",
            "\n",
            "epoch 10 loss  0.0008\n",
            "time taken for 1 epoch 4.1562981605529785 sec\n",
            "\n",
            "epoch 10 loss  0.0008\n",
            "time taken for 1 epoch 4.4639668464660645 sec\n",
            "\n",
            "epoch 10 loss  0.0009\n",
            "time taken for 1 epoch 4.769392728805542 sec\n",
            "\n",
            "epoch 10 loss  0.0009\n",
            "time taken for 1 epoch 5.056551694869995 sec\n",
            "\n",
            "epoch 10 loss  0.0010\n",
            "time taken for 1 epoch 5.337366104125977 sec\n",
            "\n",
            "epoch 10 loss  0.0010\n",
            "time taken for 1 epoch 5.630766868591309 sec\n",
            "\n",
            "epoch 10 loss  0.0011\n",
            "time taken for 1 epoch 5.935470104217529 sec\n",
            "\n",
            "epoch 10 batch 20 loss   0.8205\n",
            "epoch 10 loss  0.0011\n",
            "time taken for 1 epoch 6.220823526382446 sec\n",
            "\n",
            "epoch 10 loss  0.0012\n",
            "time taken for 1 epoch 6.504274368286133 sec\n",
            "\n",
            "epoch 10 loss  0.0012\n",
            "time taken for 1 epoch 6.801775217056274 sec\n",
            "\n",
            "epoch 10 loss  0.0013\n",
            "time taken for 1 epoch 7.0928521156311035 sec\n",
            "\n",
            "epoch 10 loss  0.0013\n",
            "time taken for 1 epoch 7.3891212940216064 sec\n",
            "\n",
            "epoch 10 loss  0.0014\n",
            "time taken for 1 epoch 7.681951522827148 sec\n",
            "\n",
            "epoch 10 loss  0.0014\n",
            "time taken for 1 epoch 7.962664365768433 sec\n",
            "\n",
            "epoch 10 loss  0.0015\n",
            "time taken for 1 epoch 8.247026443481445 sec\n",
            "\n",
            "epoch 10 loss  0.0015\n",
            "time taken for 1 epoch 8.53721809387207 sec\n",
            "\n",
            "epoch 10 loss  0.0016\n",
            "time taken for 1 epoch 8.820174217224121 sec\n",
            "\n",
            "epoch 10 batch 30 loss   0.9108\n",
            "epoch 10 loss  0.0016\n",
            "time taken for 1 epoch 9.104633808135986 sec\n",
            "\n",
            "epoch 10 loss  0.0017\n",
            "time taken for 1 epoch 9.398835897445679 sec\n",
            "\n",
            "epoch 10 loss  0.0017\n",
            "time taken for 1 epoch 9.692260980606079 sec\n",
            "\n",
            "epoch 10 loss  0.0018\n",
            "time taken for 1 epoch 9.987432479858398 sec\n",
            "\n",
            "epoch 10 loss  0.0018\n",
            "time taken for 1 epoch 10.269694805145264 sec\n",
            "\n",
            "epoch 10 loss  0.0019\n",
            "time taken for 1 epoch 10.53851866722107 sec\n",
            "\n",
            "epoch 10 loss  0.0019\n",
            "time taken for 1 epoch 10.832363605499268 sec\n",
            "\n",
            "epoch 10 loss  0.0020\n",
            "time taken for 1 epoch 11.121260404586792 sec\n",
            "\n",
            "epoch 10 loss  0.0021\n",
            "time taken for 1 epoch 11.413672685623169 sec\n",
            "\n",
            "epoch 10 loss  0.0021\n",
            "time taken for 1 epoch 11.705902576446533 sec\n",
            "\n",
            "epoch 10 batch 40 loss   0.7858\n",
            "epoch 10 loss  0.0022\n",
            "time taken for 1 epoch 11.99547529220581 sec\n",
            "\n",
            "epoch 10 loss  0.0022\n",
            "time taken for 1 epoch 12.28270435333252 sec\n",
            "\n",
            "epoch 10 loss  0.0023\n",
            "time taken for 1 epoch 12.566381454467773 sec\n",
            "\n",
            "epoch 10 loss  0.0023\n",
            "time taken for 1 epoch 12.859508275985718 sec\n",
            "\n",
            "epoch 10 loss  0.0024\n",
            "time taken for 1 epoch 13.143050193786621 sec\n",
            "\n",
            "epoch 10 loss  0.0024\n",
            "time taken for 1 epoch 13.425368785858154 sec\n",
            "\n",
            "epoch 10 loss  0.0025\n",
            "time taken for 1 epoch 13.716411828994751 sec\n",
            "\n",
            "epoch 10 loss  0.0025\n",
            "time taken for 1 epoch 14.0076584815979 sec\n",
            "\n",
            "epoch 10 loss  0.0026\n",
            "time taken for 1 epoch 14.288952112197876 sec\n",
            "\n",
            "epoch 10 loss  0.0026\n",
            "time taken for 1 epoch 14.575596570968628 sec\n",
            "\n",
            "epoch 10 batch 50 loss   0.9053\n",
            "epoch 10 loss  0.0027\n",
            "time taken for 1 epoch 14.869181632995605 sec\n",
            "\n",
            "epoch 10 loss  0.0028\n",
            "time taken for 1 epoch 15.146420240402222 sec\n",
            "\n",
            "epoch 10 loss  0.0028\n",
            "time taken for 1 epoch 15.424972772598267 sec\n",
            "\n",
            "epoch 10 loss  0.0029\n",
            "time taken for 1 epoch 15.703223466873169 sec\n",
            "\n",
            "epoch 10 loss  0.0029\n",
            "time taken for 1 epoch 15.985500812530518 sec\n",
            "\n",
            "epoch 10 loss  0.0030\n",
            "time taken for 1 epoch 16.276020288467407 sec\n",
            "\n",
            "epoch 10 loss  0.0030\n",
            "time taken for 1 epoch 16.560267210006714 sec\n",
            "\n",
            "epoch 10 loss  0.0031\n",
            "time taken for 1 epoch 16.8466739654541 sec\n",
            "\n",
            "epoch 10 loss  0.0031\n",
            "time taken for 1 epoch 17.141072511672974 sec\n",
            "\n",
            "epoch 10 loss  0.0032\n",
            "time taken for 1 epoch 17.42317032814026 sec\n",
            "\n",
            "epoch 10 batch 60 loss   0.8517\n",
            "epoch 10 loss  0.0032\n",
            "time taken for 1 epoch 17.709357500076294 sec\n",
            "\n",
            "epoch 10 loss  0.0033\n",
            "time taken for 1 epoch 17.996828317642212 sec\n",
            "\n",
            "epoch 10 loss  0.0033\n",
            "time taken for 1 epoch 18.28083348274231 sec\n",
            "\n",
            "epoch 10 loss  0.0034\n",
            "time taken for 1 epoch 18.570706129074097 sec\n",
            "\n",
            "epoch 10 loss  0.0034\n",
            "time taken for 1 epoch 18.851953268051147 sec\n",
            "\n",
            "epoch 10 loss  0.0035\n",
            "time taken for 1 epoch 19.141733407974243 sec\n",
            "\n",
            "epoch 10 loss  0.0035\n",
            "time taken for 1 epoch 19.421535968780518 sec\n",
            "\n",
            "epoch 10 loss  0.0036\n",
            "time taken for 1 epoch 19.699002265930176 sec\n",
            "\n",
            "epoch 10 loss  0.0036\n",
            "time taken for 1 epoch 19.98023796081543 sec\n",
            "\n",
            "epoch 10 loss  0.0037\n",
            "time taken for 1 epoch 20.263442039489746 sec\n",
            "\n",
            "epoch 10 batch 70 loss   0.8578\n",
            "epoch 10 loss  0.0037\n",
            "time taken for 1 epoch 20.553086757659912 sec\n",
            "\n",
            "epoch 10 loss  0.0038\n",
            "time taken for 1 epoch 20.833736419677734 sec\n",
            "\n",
            "epoch 10 loss  0.0039\n",
            "time taken for 1 epoch 21.124828815460205 sec\n",
            "\n",
            "epoch 10 loss  0.0039\n",
            "time taken for 1 epoch 21.408518075942993 sec\n",
            "\n",
            "epoch 10 loss  0.0040\n",
            "time taken for 1 epoch 21.69803023338318 sec\n",
            "\n",
            "epoch 10 loss  0.0040\n",
            "time taken for 1 epoch 21.98573660850525 sec\n",
            "\n",
            "epoch 10 loss  0.0041\n",
            "time taken for 1 epoch 22.272910594940186 sec\n",
            "\n",
            "epoch 10 loss  0.0041\n",
            "time taken for 1 epoch 22.565237283706665 sec\n",
            "\n",
            "epoch 10 loss  0.0042\n",
            "time taken for 1 epoch 22.84847855567932 sec\n",
            "\n",
            "epoch 10 loss  0.0042\n",
            "time taken for 1 epoch 23.13861083984375 sec\n",
            "\n",
            "epoch 10 batch 80 loss   0.8122\n",
            "epoch 10 loss  0.0043\n",
            "time taken for 1 epoch 23.420097589492798 sec\n",
            "\n",
            "epoch 10 loss  0.0043\n",
            "time taken for 1 epoch 23.71267342567444 sec\n",
            "\n",
            "epoch 10 loss  0.0044\n",
            "time taken for 1 epoch 23.995386362075806 sec\n",
            "\n",
            "epoch 10 loss  0.0044\n",
            "time taken for 1 epoch 24.284364223480225 sec\n",
            "\n",
            "epoch 10 loss  0.0045\n",
            "time taken for 1 epoch 24.592196226119995 sec\n",
            "\n",
            "epoch 10 loss  0.0045\n",
            "time taken for 1 epoch 24.887983322143555 sec\n",
            "\n",
            "epoch 10 loss  0.0046\n",
            "time taken for 1 epoch 25.167309999465942 sec\n",
            "\n",
            "epoch 10 loss  0.0046\n",
            "time taken for 1 epoch 25.456227779388428 sec\n",
            "\n",
            "epoch 10 loss  0.0047\n",
            "time taken for 1 epoch 25.75816583633423 sec\n",
            "\n",
            "epoch 10 loss  0.0047\n",
            "time taken for 1 epoch 26.043392658233643 sec\n",
            "\n",
            "epoch 10 batch 90 loss   0.7576\n",
            "epoch 10 loss  0.0048\n",
            "time taken for 1 epoch 26.324561834335327 sec\n",
            "\n",
            "epoch 10 loss  0.0048\n",
            "time taken for 1 epoch 26.614240169525146 sec\n",
            "\n",
            "epoch 10 loss  0.0049\n",
            "time taken for 1 epoch 26.909425497055054 sec\n",
            "\n",
            "epoch 10 loss  0.0049\n",
            "time taken for 1 epoch 27.187310218811035 sec\n",
            "\n",
            "epoch 10 loss  0.0050\n",
            "time taken for 1 epoch 27.4783296585083 sec\n",
            "\n",
            "epoch 10 loss  0.0050\n",
            "time taken for 1 epoch 27.774872303009033 sec\n",
            "\n",
            "epoch 10 loss  0.0051\n",
            "time taken for 1 epoch 28.065498113632202 sec\n",
            "\n",
            "epoch 10 loss  0.0051\n",
            "time taken for 1 epoch 28.35015606880188 sec\n",
            "\n",
            "epoch 10 loss  0.0052\n",
            "time taken for 1 epoch 28.656562328338623 sec\n",
            "\n",
            "epoch 10 loss  0.0052\n",
            "time taken for 1 epoch 28.956223011016846 sec\n",
            "\n",
            "epoch 10 batch 100 loss   0.8705\n",
            "epoch 10 loss  0.0053\n",
            "time taken for 1 epoch 29.255707025527954 sec\n",
            "\n",
            "epoch 10 loss  0.0053\n",
            "time taken for 1 epoch 29.539042711257935 sec\n",
            "\n",
            "epoch 10 loss  0.0054\n",
            "time taken for 1 epoch 29.84446358680725 sec\n",
            "\n",
            "epoch 10 loss  0.0055\n",
            "time taken for 1 epoch 30.135901927947998 sec\n",
            "\n",
            "epoch 10 loss  0.0055\n",
            "time taken for 1 epoch 30.43816614151001 sec\n",
            "\n",
            "epoch 10 loss  0.0056\n",
            "time taken for 1 epoch 30.73428702354431 sec\n",
            "\n",
            "epoch 10 loss  0.0056\n",
            "time taken for 1 epoch 31.034268617630005 sec\n",
            "\n",
            "epoch 10 loss  0.0057\n",
            "time taken for 1 epoch 31.323925971984863 sec\n",
            "\n",
            "epoch 10 loss  0.0057\n",
            "time taken for 1 epoch 31.628862619400024 sec\n",
            "\n",
            "epoch 10 loss  0.0058\n",
            "time taken for 1 epoch 31.915395259857178 sec\n",
            "\n",
            "epoch 10 batch 110 loss   0.8447\n",
            "epoch 10 loss  0.0058\n",
            "time taken for 1 epoch 32.207767486572266 sec\n",
            "\n",
            "epoch 10 loss  0.0059\n",
            "time taken for 1 epoch 32.50940251350403 sec\n",
            "\n",
            "epoch 10 loss  0.0059\n",
            "time taken for 1 epoch 32.80106544494629 sec\n",
            "\n",
            "epoch 10 loss  0.0059\n",
            "time taken for 1 epoch 33.10811233520508 sec\n",
            "\n",
            "epoch 10 loss  0.0060\n",
            "time taken for 1 epoch 33.41319298744202 sec\n",
            "\n",
            "epoch 10 loss  0.0061\n",
            "time taken for 1 epoch 33.702306032180786 sec\n",
            "\n",
            "epoch 10 loss  0.0061\n",
            "time taken for 1 epoch 34.00380301475525 sec\n",
            "\n",
            "epoch 10 loss  0.0062\n",
            "time taken for 1 epoch 34.28462219238281 sec\n",
            "\n",
            "epoch 10 loss  0.0062\n",
            "time taken for 1 epoch 34.57411551475525 sec\n",
            "\n",
            "epoch 10 loss  0.0063\n",
            "time taken for 1 epoch 34.87703061103821 sec\n",
            "\n",
            "epoch 10 batch 120 loss   0.7971\n",
            "epoch 10 loss  0.0063\n",
            "time taken for 1 epoch 35.182886600494385 sec\n",
            "\n",
            "epoch 10 loss  0.0064\n",
            "time taken for 1 epoch 35.464014530181885 sec\n",
            "\n",
            "epoch 10 loss  0.0064\n",
            "time taken for 1 epoch 35.76051044464111 sec\n",
            "\n",
            "epoch 10 loss  0.0064\n",
            "time taken for 1 epoch 36.05926322937012 sec\n",
            "\n",
            "epoch 10 loss  0.0065\n",
            "time taken for 1 epoch 36.34935545921326 sec\n",
            "\n",
            "epoch 11 batch 0 loss   0.7721\n",
            "epoch 11 batch 10 loss   0.8119\n",
            "epoch 11 batch 20 loss   0.7804\n",
            "epoch 11 batch 30 loss   0.8618\n",
            "epoch 11 batch 40 loss   0.7444\n",
            "epoch 11 batch 50 loss   0.8315\n",
            "epoch 11 batch 60 loss   0.7909\n",
            "epoch 11 batch 70 loss   0.8185\n",
            "epoch 11 batch 80 loss   0.7590\n",
            "epoch 11 batch 90 loss   0.7141\n",
            "epoch 11 batch 100 loss   0.8107\n",
            "epoch 11 batch 110 loss   0.7789\n",
            "epoch 11 batch 120 loss   0.7288\n",
            "epoch 12 batch 0 loss   0.7294\n",
            "epoch 12 batch 10 loss   0.7743\n",
            "epoch 12 batch 20 loss   0.7232\n",
            "epoch 12 batch 30 loss   0.7872\n",
            "epoch 12 batch 40 loss   0.7039\n",
            "epoch 12 batch 50 loss   0.7742\n",
            "epoch 12 batch 60 loss   0.7516\n",
            "epoch 12 batch 70 loss   0.7726\n",
            "epoch 12 batch 80 loss   0.7104\n",
            "epoch 12 batch 90 loss   0.6881\n",
            "epoch 12 batch 100 loss   0.7602\n",
            "epoch 12 batch 110 loss   0.7161\n",
            "epoch 12 batch 120 loss   0.6879\n",
            "epoch 13 batch 0 loss   0.6818\n",
            "epoch 13 batch 10 loss   0.7485\n",
            "epoch 13 batch 20 loss   0.7046\n",
            "epoch 13 batch 30 loss   0.7796\n",
            "epoch 13 batch 40 loss   0.6758\n",
            "epoch 13 batch 50 loss   0.7491\n",
            "epoch 13 batch 60 loss   0.7289\n",
            "epoch 13 batch 70 loss   0.7349\n",
            "epoch 13 batch 80 loss   0.6793\n",
            "epoch 13 batch 90 loss   0.6282\n",
            "epoch 13 batch 100 loss   0.7973\n",
            "epoch 13 batch 110 loss   0.7458\n",
            "epoch 13 batch 120 loss   0.7206\n",
            "epoch 14 batch 0 loss   0.6759\n",
            "epoch 14 batch 10 loss   0.7485\n",
            "epoch 14 batch 20 loss   0.7561\n",
            "epoch 14 batch 30 loss   0.8537\n",
            "epoch 14 batch 40 loss   0.7385\n",
            "epoch 14 batch 50 loss   0.8976\n",
            "epoch 14 batch 60 loss   0.8108\n",
            "epoch 14 batch 70 loss   0.8462\n",
            "epoch 14 batch 80 loss   0.8032\n",
            "epoch 14 batch 90 loss   0.7409\n",
            "epoch 14 batch 100 loss   0.8201\n",
            "epoch 14 batch 110 loss   0.7735\n",
            "epoch 14 batch 120 loss   0.7561\n",
            "epoch 15 batch 0 loss   0.7329\n",
            "epoch 15 batch 10 loss   0.7553\n",
            "epoch 15 batch 20 loss   0.7453\n",
            "epoch 15 batch 30 loss   0.7337\n",
            "epoch 15 batch 40 loss   0.6850\n",
            "epoch 15 batch 50 loss   0.8119\n",
            "epoch 15 batch 60 loss   0.7879\n",
            "epoch 15 batch 70 loss   0.7843\n",
            "epoch 15 batch 80 loss   0.7346\n",
            "epoch 15 batch 90 loss   0.7088\n",
            "epoch 15 batch 100 loss   0.8068\n",
            "epoch 15 batch 110 loss   0.7615\n",
            "epoch 15 batch 120 loss   0.7619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpcAbJ0DWQcG",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWHNmjCEWQcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(en_text):\n",
        "    # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = expand_constraction(en_text)\n",
        "        en_text = replace_special_character_to_space(en_text)\n",
        "        en_text = en_text.rstrip().strip()\n",
        "\n",
        "        en_text = \"<sos> \" + en_text + \" <eos>\"\n",
        "        return en_text\n",
        "        \n",
        "\n",
        "def evaluate(sentence):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['<sos>']], 0)\n",
        "\n",
        "    for t in range(max_length_target):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += target_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_lang.index_word[predicted_id] == '<eos>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "            # pred → predictions\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNrfTIcWQcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh5SvoXVWQcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH9wydYsWQcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "fe066425-e611-4336-dc31-a8a4a5a11874"
      },
      "source": [
        "translate(\"yes\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> yes <eos>\n",
            "Predicted translation: ああ <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAJiCAYAAAAWmcxQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGCI8AIAGCK8AACGCC8AgCHCCwBgyP8DqIaTUxOPLVUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D_G1CJkWQcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}