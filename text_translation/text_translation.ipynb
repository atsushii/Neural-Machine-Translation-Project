{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "def load_def(filename):\n",
    "    # open a txt file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    lines = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    text = load_def(\"dataset/raw.txt\")\n",
    "\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate English to Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    # avoid lang accent check → japanese has lang acccent\n",
    "    return ' '.join(c for c in unicodedata.normalize('NFC', s))\n",
    "\n",
    "# preprocess\n",
    "def preprocess_sentence(w):\n",
    "    # Morphological analysis for japanese lang\n",
    "    m = MeCab.Tagger(\"-Owakati\")\n",
    "    w = m.parse(w)\n",
    "    \n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # create a space between word and the punctuation\n",
    "    w = re.sub(r\"([?!¿.,。])\", r\" \\1 \", w)\n",
    "    # replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "    w = re.sub(r\"[^a-zA-Z\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?.!,。¿\\-/ {1,}/]+\",  \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add a start and end  token to the sentence\n",
    "    # model know when to start and end\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> m a y   i   b o r r o w   t h i s   b o o k    ? <end>\n",
      "<start> プ ー ル   に   行 き   た い    。    で も   今 日   は   雨 <end>\n"
     ]
    }
   ],
   "source": [
    "# check word\n",
    "en_sentence =u\"May I borrow this book?\"\n",
    "ja_sentence = u\"プールに行きたい。でも今日は雨\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ja_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> y o u   a r e   b a c k    ,    a r e n       t   y o u    ,    h a r o l d    ? <end>', '<start> m y   o p p o n e n t   i s   s h a r k    . <end>', '<start> t h i s   i s   o n e   t h i n g   i n   e x c h a n g e   f o r   a n o t h e r    . <end>', '<start> y e a h    ,    i       m   f i n e    . <end>', '<start> d o n       t   c o m e   t o   t h e   o f f i c e   a n y m o r e    .    d o n       t   c a l l   m e   e i t h e r    . <end>', '<start> l o o k s   b e a u t i f u l    . <end>', '<start> g e t   h i m   o u t   o f   h e r e    ,    b e c a u s e   i   w i l l   f u c k i n g   k i l l   h i m    . <end>', '<start> y o u   k i l l e d   h i m    ! <end>', '<start> o k a y    ,    t h e n   w h o    ? <end>', '<start> i t   s e e m s   a   f o r m e r   e m p l o y e e    .   .   . <end>')\n",
      "('<start> あ な た   は   戻 っ   た   の   ね   ハ ロ ル ド    ? <end>', '<start> 俺   の   相 手   は   シ ャ ー ク   だ    。 <end>', '<start> 引 き 換 え   だ   あ る   事   と   あ る   物   の <end>', '<start> も う   い い   よ   ご ち そ う さ ま   う う ん <end>', '<start> も う   会 社   に   は   来   な い   で   く れ   電 話   も   す る   な <end>', '<start> き れ い   だ    。 <end>', '<start> 連 れ   て   行 け   殺 し   そ う   だ   わ か っ   た   か    ? <end>', '<start> 殺 し   た   の   か    ! <end>', '<start> わ ぁ      !    い つ も   す み ま せ ん    。    い い   の   よ      。 <end>', '<start> カ ン パ ニ ー   の   元   社 員   が <end>')\n"
     ]
    }
   ],
   "source": [
    "en, ja = create_lang_list(10)\n",
    "print(en)\n",
    "print(ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus     \n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:1, place:2, is:3, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # transform a list of num sample into a 2D Numpy array of shape \n",
    "    # Fixed length because length of sequence of integers are different\n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3, 4, 5, 6]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x1a2f437828>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokenize([\"this place is good haha hey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean input, output pairs\n",
    "def load_dataset(num_example):\n",
    "    input_lang, target_lang= create_lang_list(num_example)\n",
    "    input_tensor, input_lang_tokenize = tokenize(input_lang)\n",
    "    target_tensor, target_lang_tokenize = tokenize(target_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit datasize for test\n",
    "num_example = 30000\n",
    "# get data\n",
    "input_tensor, target_tensor, input_lang, target_lang = load_dataset(num_example)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000 24000 6000\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "input_tensor_train, input_tensor_val, \\\n",
    "    target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# show length\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "12-----><start>\n",
      "8----->h\n",
      "1----->e\n",
      "14----->d\n",
      "5----->i\n",
      "14----->d\n",
      "5----->i\n",
      "2----->t\n",
      "18----->w\n",
      "5----->i\n",
      "2----->t\n",
      "8----->h\n",
      "8----->h\n",
      "5----->i\n",
      "7----->s\n",
      "3----->o\n",
      "18----->w\n",
      "6----->n\n",
      "18----->w\n",
      "5----->i\n",
      "10----->l\n",
      "10----->l\n",
      "28----->!\n",
      "13-----><end>\n",
      "output lang: index to word mapping\n",
      "1-----><start>\n",
      "2139----->茂\n",
      "16----->が\n",
      "442----->勝\n",
      "104----->手\n",
      "10----->に\n",
      "364----->落\n",
      "45----->ち\n",
      "6----->た\n",
      "3----->の\n",
      "2-----><end>\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(input_lang, input_tensor_train[0])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(target_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((255,), (80,)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 255), (64, 80)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# create a dataset\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang.word_index) + 1\n",
    "vocab_tar_size = len(target_lang.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "print(dataset)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/miyamotoatsushi/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(64), Dimension(255)]),\n",
       " TensorShape([Dimension(64), Dimension(80)]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch =  next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove\n",
    "def load_glove():\n",
    "    embeddings_dictionary = {}\n",
    "    glove_file =  open()\n",
    "    \n",
    "    for line in glove_file:\n",
    "        records =  line.split()\n",
    "        word = records[0]\n",
    "        vector_ dimensions = asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] =  vector_dimensions\n",
    "    glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
